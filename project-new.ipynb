{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.ml import feature as MF\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"etl_project\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('train.csv',header=True,inferSchema=True)\n",
    "# df_test = spark.read.csv('test.csv',header=True,inferSchema=True)\n",
    "# sample_sub = spark.read.csv('sample_submission.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Display First Few Rows of Training Data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Row(id=0, Age=19.0, Gender='Female', Annual Income=10049.0, Marital Status='Married', Number of Dependents=1.0, Education Level=\"Bachelor's\", Occupation='Self-Employed', Health Score=22.59876067181393, Location='Urban', Policy Type='Premium', Previous Claims=2.0, Vehicle Age=17.0, Credit Score=372.0, Insurance Duration=5.0, Policy Start Date=datetime.datetime(2023, 12, 23, 15, 21, 39, 134960), Customer Feedback='Poor', Smoking Status='No', Exercise Frequency='Weekly', Property Type='House', Premium Amount=2869.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"Display First Few Rows of Training Data\", df_train.head())\n",
    "# display(\"Display First Few Rows of Testing Data\", df_test.head())\n",
    "# display(\"Display First Few Rows of Sample Submission\", sample_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the Training Data\n",
      "+-------+-----------------+------------------+-------+-----------------+--------------+--------------------+---------------+----------+------------------+--------+-----------+------------------+-----------------+-----------------+------------------+-----------------+--------------+------------------+-------------+------------------+\n",
      "|summary|               id|               Age| Gender|    Annual Income|Marital Status|Number of Dependents|Education Level|Occupation|      Health Score|Location|Policy Type|   Previous Claims|      Vehicle Age|     Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|    Premium Amount|\n",
      "+-------+-----------------+------------------+-------+-----------------+--------------+--------------------+---------------+----------+------------------+--------+-----------+------------------+-----------------+-----------------+------------------+-----------------+--------------+------------------+-------------+------------------+\n",
      "|  count|          1200000|           1181295|1200000|          1155051|       1181471|             1090328|        1200000|    841925|           1125924| 1200000|    1200000|            835971|          1199994|          1062118|           1199999|          1122176|       1200000|           1200000|      1200000|           1200000|\n",
      "|   mean|         599999.5|41.145563131986506|   NULL|32745.21777653108|          NULL|  2.0099337080218063|           NULL|      NULL| 25.61390769251917|    NULL|       NULL| 1.002689088497089|9.569888682776748|592.9243502134415| 5.018219181849318|             NULL|          NULL|              NULL|         NULL|1102.5448216666666|\n",
      "| stddev|346410.3058513127|13.539949792477247|   NULL|32179.50612417942|          NULL|  1.4173377664726148|           NULL|      NULL|12.203461881250199|    NULL|       NULL|0.9828401709548069|5.776188677537853|149.9819452922619| 2.594331342102338|             NULL|          NULL|              NULL|         NULL| 864.9988587527372|\n",
      "|    min|                0|              18.0| Female|              1.0|      Divorced|                 0.0|     Bachelor's|  Employed|2.0122371818911766|   Rural|      Basic|               0.0|              0.0|            300.0|               1.0|          Average|            No|             Daily|    Apartment|              20.0|\n",
      "|    max|          1199999|              64.0|   Male|         149997.0|        Single|                 4.0|            PhD|Unemployed| 58.97591405405534|   Urban|    Premium|               9.0|             19.0|            849.0|               9.0|             Poor|           Yes|            Weekly|        House|            4999.0|\n",
      "+-------+-----------------+------------------+-------+-----------------+--------------+--------------------+---------------+----------+------------------+--------+-----------+------------------+-----------------+-----------------+------------------+-----------------+--------------+------------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary of the Training Data\")\n",
    "df_train.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of Training Data (%):\n",
      "+---+-------+------+------------------+------------------+--------------------+---------------+------------------+------------+--------+-----------+---------------+-----------+------------------+--------------------+-----------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id|    Age|Gender|     Annual Income|    Marital Status|Number of Dependents|Education Level|        Occupation|Health Score|Location|Policy Type|Previous Claims|Vehicle Age|      Credit Score|  Insurance Duration|Policy Start Date|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+-------+------+------------------+------------------+--------------------+---------------+------------------+------------+--------+-----------+---------------+-----------+------------------+--------------------+-----------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|0.0|1.55875|   0.0|3.7457499999999997|1.5440833333333335|   9.139333333333333|            0.0|29.839583333333337|       6.173|     0.0|        0.0|       30.33575|     5.0E-4|11.490166666666667|8.333333333333333E-5|              0.0|6.485333333333333|           0.0|               0.0|          0.0|           0.0|\n",
      "+---+-------+------+------------------+------------------+--------------------+---------------+------------------+------------+--------+-----------+---------------+-----------+------------------+--------------------+-----------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Missing values of Training Data:\\n\", df_train.isnull().sum()/len(df_train)*100)  #pandas\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "print(\"Missing values of Training Data (%):\")\n",
    "df_train.select(\n",
    "    [(sum(col(c).isNull().cast(\"int\")) / df_train.count() * 100).alias(c) for c in df_train.columns]\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|   Occupation|      Health Score|Location|  Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|   Policy Start Date|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|Self-Employed| 22.59876067181393|   Urban|      Premium|            2.0|       17.0|       372.0|               5.0|2023-12-23 15:21:...|             Poor|            No|            Weekly|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|         NULL|15.569730989408043|   Rural|Comprehensive|            1.0|       12.0|       694.0|               2.0|2023-06-12 15:21:...|          Average|           Yes|           Monthly|        House|        1483.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|Self-Employed| 47.17754928786464|Suburban|      Premium|            1.0|       14.0|        NULL|               3.0|2023-09-30 15:21:...|             Good|           Yes|            Weekly|        House|         567.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|         NULL|10.938144158664583|   Rural|        Basic|            1.0|        0.0|       367.0|               1.0|2024-06-12 15:21:...|             Poor|           Yes|             Daily|    Apartment|         765.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|Self-Employed|20.376093627736925|   Rural|      Premium|            0.0|        8.0|       598.0|               4.0|2021-12-01 15:21:...|             Poor|           Yes|            Weekly|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|         NULL| 33.05319768402281|   Urban|      Premium|            2.0|        4.0|       614.0|               5.0|2022-05-20 15:21:...|          Average|            No|            Weekly|        House|        3202.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|         NULL|              NULL|   Rural|        Basic|            2.0|        8.0|       807.0|               6.0|2020-02-21 15:21:...|             Poor|            No|            Weekly|        House|         439.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|     Employed| 5.769783092512088|Suburban|Comprehensive|            1.0|       11.0|       398.0|               5.0|2022-08-08 15:21:...|          Average|            No|            Rarely|        Condo|         111.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('Age', 'double'),\n",
       " ('Gender', 'string'),\n",
       " ('Annual Income', 'double'),\n",
       " ('Marital Status', 'string'),\n",
       " ('Number of Dependents', 'double'),\n",
       " ('Education Level', 'string'),\n",
       " ('Occupation', 'string'),\n",
       " ('Health Score', 'double'),\n",
       " ('Location', 'string'),\n",
       " ('Policy Type', 'string'),\n",
       " ('Previous Claims', 'double'),\n",
       " ('Vehicle Age', 'double'),\n",
       " ('Credit Score', 'double'),\n",
       " ('Insurance Duration', 'double'),\n",
       " ('Policy Start Date', 'timestamp'),\n",
       " ('Customer Feedback', 'string'),\n",
       " ('Smoking Status', 'string'),\n",
       " ('Exercise Frequency', 'string'),\n",
       " ('Property Type', 'string'),\n",
       " ('Premium Amount', 'double')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# df_train.select('Credit Score').show()\n",
    "df_train.select(F.min(\"Credit Score\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|summary|Credit Score|\n",
      "+-------+------------+\n",
      "|    min|       300.0|\n",
      "|    25%|       468.0|\n",
      "|    50%|       595.0|\n",
      "|    75%|       721.0|\n",
      "|    max|       849.0|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.select(\"Credit Score\").summary(\"min\", \"25%\", \"50%\", \"75%\", \"max\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, id: string, Age: string, Gender: string, Annual Income: string, Marital Status: string, Number of Dependents: string, Education Level: string, Occupation: string, Health Score: string, Location: string, Policy Type: string, Previous Claims: string, Vehicle Age: string, Credit Score: string, Insurance Duration: string, Customer Feedback: string, Smoking Status: string, Exercise Frequency: string, Property Type: string, Premium Amount: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  main concerns about handling null\n",
    "\n",
    "> keep the null and then string index it\n",
    "\n",
    "> fill the null but how? \n",
    "\n",
    "> drop the null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Gender|Frequency|\n",
      "+------+---------+\n",
      "|Female|   597429|\n",
      "|  Male|   602571|\n",
      "+------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+--------------+---------+\n",
      "|Marital Status|Frequency|\n",
      "+--------------+---------+\n",
      "|       Married|   394316|\n",
      "|      Divorced|   391764|\n",
      "|          NULL|    18529|\n",
      "|        Single|   395391|\n",
      "+--------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+--------------------+---------+\n",
      "|Number of Dependents|Frequency|\n",
      "+--------------------+---------+\n",
      "|                 0.0|   218124|\n",
      "|                 1.0|   215076|\n",
      "|                 3.0|   221475|\n",
      "|                 4.0|   220340|\n",
      "|                NULL|   109672|\n",
      "|                 2.0|   215313|\n",
      "+--------------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+---------------+---------+\n",
      "|Education Level|Frequency|\n",
      "+---------------+---------+\n",
      "|    High School|   289441|\n",
      "|            PhD|   303507|\n",
      "|       Master's|   303818|\n",
      "|     Bachelor's|   303234|\n",
      "+---------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+-------------+---------+\n",
      "|   Occupation|Frequency|\n",
      "+-------------+---------+\n",
      "|     Employed|   282750|\n",
      "|   Unemployed|   276530|\n",
      "|         NULL|   358075|\n",
      "|Self-Employed|   282645|\n",
      "+-------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+--------+---------+\n",
      "|Location|Frequency|\n",
      "+--------+---------+\n",
      "|   Urban|   397511|\n",
      "|   Rural|   400947|\n",
      "|Suburban|   401542|\n",
      "+--------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+-------------+---------+\n",
      "|  Policy Type|Frequency|\n",
      "+-------------+---------+\n",
      "|      Premium|   401846|\n",
      "|Comprehensive|   399600|\n",
      "|        Basic|   398554|\n",
      "+-------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+---------------+---------+\n",
      "|Previous Claims|Frequency|\n",
      "+---------------+---------+\n",
      "|            6.0|      302|\n",
      "|            7.0|       58|\n",
      "|            0.0|   305433|\n",
      "|            1.0|   300811|\n",
      "|           NULL|   364029|\n",
      "|            3.0|    49011|\n",
      "|            4.0|    10668|\n",
      "|            2.0|   167661|\n",
      "|            5.0|     2018|\n",
      "|            8.0|        8|\n",
      "|            9.0|        1|\n",
      "+---------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+------------------+---------+\n",
      "|Insurance Duration|Frequency|\n",
      "+------------------+---------+\n",
      "|               6.0|   132141|\n",
      "|               7.0|   133592|\n",
      "|               3.0|   132018|\n",
      "|               1.0|   135072|\n",
      "|               4.0|   132182|\n",
      "|               5.0|   132253|\n",
      "|               2.0|   131160|\n",
      "|               8.0|   133800|\n",
      "|               9.0|   137781|\n",
      "|              NULL|        1|\n",
      "+------------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+-----------------+---------+\n",
      "|Customer Feedback|Frequency|\n",
      "+-----------------+---------+\n",
      "|          Average|   377905|\n",
      "|             Poor|   375518|\n",
      "|             Good|   368753|\n",
      "|             NULL|    77824|\n",
      "+-----------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+--------------+---------+\n",
      "|Smoking Status|Frequency|\n",
      "+--------------+---------+\n",
      "|            No|   598127|\n",
      "|           Yes|   601873|\n",
      "+--------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+------------------+---------+\n",
      "|Exercise Frequency|Frequency|\n",
      "+------------------+---------+\n",
      "|            Weekly|   306179|\n",
      "|             Daily|   294571|\n",
      "|            Rarely|   299420|\n",
      "|           Monthly|   299830|\n",
      "+------------------+---------+\n",
      "\n",
      "===================\n",
      "\n",
      "+-------------+---------+\n",
      "|Property Type|Frequency|\n",
      "+-------------+---------+\n",
      "|        House|   400349|\n",
      "|    Apartment|   399978|\n",
      "|        Condo|   399673|\n",
      "+-------------+---------+\n",
      "\n",
      "===================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  count\n",
    "\n",
    "# column_names  = ['Gender',  'Marital Status']\n",
    "column_names = ['Gender', 'Marital Status', 'Number of Dependents', 'Education Level', 'Occupation',  'Location', 'Policy Type', 'Previous Claims',   'Insurance Duration',  'Customer Feedback', 'Smoking Status', 'Exercise Frequency', 'Property Type']\n",
    "\n",
    "# Count frequency of each value\n",
    "for column in column_names:\n",
    "\n",
    "    df_freq = df_train.groupBy(column).agg(count(\"*\").alias(\"Frequency\"))\n",
    "    # print(f\"{column}\")\n",
    "    df_freq.show()\n",
    "    print(\"===================\\n\")\n",
    "\n",
    "# Show result\n",
    "# df_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Marital Status|Frequency|\n",
      "+--------------+---------+\n",
      "|       Married|   394316|\n",
      "|      Divorced|   391764|\n",
      "|          NULL|    18529|\n",
      "|        Single|   395391|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count occurrences of each unique string\n",
    "# counts = df_train.groupBy(\"Gender\").count()\n",
    "# print(counts)\n",
    "\n",
    "from pyspark.sql.functions import col, count\n",
    "# Count frequency of each value\n",
    "df_freq = df_train.groupBy(\"Marital Status\").agg(count(\"*\").alias(\"Frequency\"))\n",
    "\n",
    "# Show result\n",
    "df_freq.show()\n",
    "\n",
    "# df_train.groupBy(\"Gender\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# middle ai to fill null ( ocupations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the second dataframe\n",
    "df2 = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|   Occupation|      Health Score|Location|  Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|   Policy Start Date|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|Self-Employed| 22.59876067181393|   Urban|      Premium|            2.0|       17.0|       372.0|               5.0|2023-12-23 15:21:...|             Poor|            No|            Weekly|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|         NULL|15.569730989408043|   Rural|Comprehensive|            1.0|       12.0|       694.0|               2.0|2023-06-12 15:21:...|          Average|           Yes|           Monthly|        House|        1483.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|Self-Employed| 47.17754928786464|Suburban|      Premium|            1.0|       14.0|        NULL|               3.0|2023-09-30 15:21:...|             Good|           Yes|            Weekly|        House|         567.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|         NULL|10.938144158664583|   Rural|        Basic|            1.0|        0.0|       367.0|               1.0|2024-06-12 15:21:...|             Poor|           Yes|             Daily|    Apartment|         765.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|Self-Employed|20.376093627736925|   Rural|      Premium|            0.0|        8.0|       598.0|               4.0|2021-12-01 15:21:...|             Poor|           Yes|            Weekly|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|         NULL| 33.05319768402281|   Urban|      Premium|            2.0|        4.0|       614.0|               5.0|2022-05-20 15:21:...|          Average|            No|            Weekly|        House|        3202.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|         NULL|              NULL|   Rural|        Basic|            2.0|        8.0|       807.0|               6.0|2020-02-21 15:21:...|             Poor|            No|            Weekly|        House|         439.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|     Employed| 5.769783092512088|Suburban|Comprehensive|            1.0|       11.0|       398.0|               5.0|2022-08-08 15:21:...|          Average|            No|            Rarely|        Condo|         111.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Policy Start Date=datetime.datetime(2023, 12, 23, 15, 21, 39, 134960))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select(\"Policy Start Date\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filling (string) Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181295 18705\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "a= df2.filter(col(\"Age\").isNotNull()).count()\n",
    "b =df2.filter(col(\"Age\").isNull()).count()\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.dropna(subset=['Occupation'])\n",
    "\n",
    "df2 = df2.fillna({'Occupation': 'Unknown_Occupation'})\n",
    "df2 = df2.fillna({'Marital Status': 'Unknown_marital_status'})\n",
    "df2 = df2.fillna({'Customer Feedback': 'Unknown_customer_feedback'})\n",
    "# df2 = df2.fillna({'Number of Dependents': '0'})\n",
    "# df2 = df2.fillna({'Previous Claims': '0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoking status to 0 and 1\n",
    "from pyspark.sql.functions import when\n",
    "# 'Smoking Status'\n",
    "\n",
    "df2 = df2.withColumn(\"Smoking Status\", when(df2[\"Smoking Status\"] == \"Yes\", 1)\n",
    "                                      .when(df2[\"Smoking Status\"] == \"No\", 0)\n",
    "                                      .otherwise(None)\n",
    "                                      ) \n",
    "\n",
    "# df2 = df2.replace({\"Smoking Status\": {\"Yes\": 1, \"No\": 0}})\n",
    "\n",
    "\n",
    "# df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop('Policy Start Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-------------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|  Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-------------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|      Premium|            2.0|       17.0|       372.0|               5.0|             Poor|             0|            Weekly|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|Comprehensive|            1.0|       12.0|       694.0|               2.0|          Average|             1|           Monthly|        House|        1483.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|      Premium|            1.0|       14.0|        NULL|               3.0|             Good|             1|            Weekly|        House|         567.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|        Basic|            1.0|        0.0|       367.0|               1.0|             Poor|             1|             Daily|    Apartment|         765.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|      Premium|            0.0|        8.0|       598.0|               4.0|             Poor|             1|            Weekly|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|      Premium|            2.0|        4.0|       614.0|               5.0|          Average|             0|            Weekly|        House|        3202.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|              NULL|   Rural|        Basic|            2.0|        8.0|       807.0|               6.0|             Poor|             0|            Weekly|        House|         439.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|Comprehensive|            1.0|       11.0|       398.0|               5.0|          Average|             0|            Rarely|        Condo|         111.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|      Premium|            1.0|       10.0|       685.0|               8.0|          Average|             0|           Monthly|        Condo|         213.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|Comprehensive|            1.0|        9.0|       635.0|               3.0|             Poor|             0|             Daily|        Condo|          64.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-------------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|        Occupation|Frequency|\n",
      "+------------------+---------+\n",
      "|Unknown_Occupation|   358075|\n",
      "|          Employed|   282750|\n",
      "|        Unemployed|   276530|\n",
      "|     Self-Employed|   282645|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df2.show(10)\n",
    "from pyspark.sql.functions import col, count\n",
    "df_freq_occu = df2.groupBy(\"Occupation\").agg(count(\"*\").alias(\"Frequency\"))\n",
    "\n",
    "# Show result\n",
    "df_freq_occu.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now filling the (numerical) Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with 0 \n",
    "# fill_with_zero = ['Previous Claims','Number of Dependents']\n",
    "\n",
    "#Previous Claims to 0\n",
    "# df2 = df2.fillna(0,subset=[\"Previous Claims\"])\n",
    "\n",
    "#Number of Dependents\n",
    "df2 = df2.fillna(0,subset=[\"Number of Dependents\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age mean is 41.15\n",
      "Null values for Age is : 0\n",
      "Health Score mean is 25.61\n",
      "Null values for Health Score is : 0\n",
      "Annual Income mean is 32745.22\n",
      "Null values for Annual Income is : 0\n",
      "Vehicle Age mean is 9.57\n",
      "Null values for Vehicle Age is : 0\n",
      "Credit Score mean is 592.92\n",
      "Null values for Credit Score is : 0\n",
      "Insurance Duration mean is 5.02\n",
      "Null values for Insurance Duration is : 0\n"
     ]
    }
   ],
   "source": [
    "mean_cols = ['Age','Health Score','Annual Income','Vehicle Age','Credit Score','Insurance Duration'] \n",
    "\n",
    "from pyspark.sql.functions import mean, col,count\n",
    "\n",
    "def fillna_with_mean(data_frame,col_names_list):\n",
    "    df2 = data_frame\n",
    "    for col_name in col_names_list:\n",
    "        mean_value = round(df2.select(mean(col(col_name))).collect()[0][0],2)\n",
    "        print(f\"{col_name} mean is {mean_value}\")\n",
    "        df2 = df2.fillna(mean_value,subset=[col_name])\n",
    "        # full check\n",
    "        number_of_null = df2.filter(col(col_name).isNull()).count()\n",
    "        print(f\"Null values for {col_name} is : {number_of_null}\")\n",
    "        \n",
    "    return df2\n",
    "\n",
    "df2 = fillna_with_mean(df2,mean_cols)\n",
    "# outlier:\n",
    "IQR = [''] # then use (median) to replace them\n",
    "\n",
    "# mode_cols = \n",
    "\n",
    "#Annual Income > took all the values more than 20k as iqr is showing bizzare values # and less than those we replaced than with median\n",
    "\n",
    "# Now ================================================\n",
    "# Number of Dependents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of df2 Training Data (%):\n",
      "+---+---+------+-------------+--------------+--------------------+---------------+----------+------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id|Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|Occupation|Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+---+------+-------------+--------------+--------------------+---------------+----------+------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|0.0|0.0|   0.0|          0.0|           0.0|                 0.0|            0.0|       0.0|         0.0|     0.0|        0.0|       30.33575|        0.0|         0.0|               0.0|              0.0|           0.0|               0.0|          0.0|           0.0|\n",
      "+---+---+------+-------------+--------------+--------------------+---------------+----------+------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "print(\"Missing values of df2 Training Data (%):\")\n",
    "df2.select(\n",
    "    [(sum(col(c).isNull().cast(\"int\")) / df2.count() * 100).alias(c) for c in df2.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.filter(col('Annual Income').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|     Annual Income|\n",
      "+-------+------------------+\n",
      "|  count|           1200000|\n",
      "|   mean|27095.636900646812|\n",
      "| stddev| 22269.90291187683|\n",
      "|    min|               1.0|\n",
      "|    max|           96871.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we'll leave annual income for now\n",
    "\n",
    "df_train   = df2\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Step 1: Compute IQR and Median\n",
    "summary = df_train.selectExpr(\n",
    "    \"percentile_approx(`Annual Income`, 0.25) as Q1\",\n",
    "    \"percentile_approx(`Annual Income`, 0.75) as Q3\",\n",
    "    \"percentile_approx(`Annual Income`, 0.5) as Median\"\n",
    ").collect()[0]\n",
    "\n",
    "Q1 = summary[\"Q1\"]\n",
    "Q3 = summary[\"Q3\"]\n",
    "IQR = Q3 - Q1\n",
    "Lower_Bound = Q1 - 1.5 * IQR\n",
    "Upper_Bound = Q3 + 1.5 * IQR\n",
    "median_income = summary[\"Median\"]\n",
    "\n",
    "# Step 2: Replace Outliers with Median\n",
    "df_train = df_train.withColumn(\n",
    "    \"Annual Income\",\n",
    "    expr(f\"CASE WHEN `Annual Income` < {Lower_Bound} OR `Annual Income` > {Upper_Bound} THEN {median_income} ELSE `Annual Income` END\")\n",
    ")\n",
    "\n",
    "# Step 3: Fill Null Values with Median\n",
    "df_train = df_train.fillna({\"Annual Income\": median_income})\n",
    "\n",
    "# Show the final result\n",
    "df_train.select(\"Annual Income\").describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88594"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "# Count frequency of each value\n",
    "df_freq = df2.groupBy(\"Annual Income\").agg(count(\"*\").alias(\"Frequency\"))\n",
    "\n",
    "# Show result\n",
    "df_freq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_freq.show(48)\n",
    "# df_freq.filter((col(\"Annual Income\") >= 1) & (col(\"Annual Income\") <= 30)).show(30)\n",
    "# df_freq.filter((col(\"Annual Income\") <= 20000) ).count()\n",
    "df_freq.filter((col(\"Annual Income\") <= 100) ).count()\n",
    "# df_freq.filter((col(\"Frequency\") >= 1) & (col(\"Frequency\") <= 100)).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "# Identify numeric columns (int and double)\n",
    "# numeric_columns = [col_name for col_name, dtype in df.dtypes if dtype in (\"int\", \"double\")]\n",
    "# print(\"Numeric Columns:\", numeric_columns)\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "column_now ='Age'\n",
    "\n",
    "# Compute Q1, Q3, and IQR\n",
    "quantiles = df2.approxQuantile(column_now, [0.25, 0.75], 0.05)\n",
    "Q1, Q3 = quantiles[0], quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#define lower & upper bound\n",
    "lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter DataFrame to remove outliers\n",
    "df2_ann=df2.filter((col(column_now) >= lower_bound) & (col(column_now) <= upper_bound))\n",
    "\n",
    "# Apply outlier removal for each numeric column\n",
    "# for col_name in numeric_columns:\n",
    "#     df = remove_outliers(df, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_ann.count()\n",
    "# df2_ann.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sometimes the numerical values also satys as sting to we didn't try to find the colunms using code which are type (sting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('Age', 'double'),\n",
       " ('Gender', 'string'),\n",
       " ('Annual Income', 'double'),\n",
       " ('Marital Status', 'string'),\n",
       " ('Number of Dependents', 'double'),\n",
       " ('Education Level', 'string'),\n",
       " ('Occupation', 'string'),\n",
       " ('Health Score', 'double'),\n",
       " ('Location', 'string'),\n",
       " ('Policy Type', 'string'),\n",
       " ('Previous Claims', 'double'),\n",
       " ('Vehicle Age', 'double'),\n",
       " ('Credit Score', 'double'),\n",
       " ('Insurance Duration', 'double'),\n",
       " ('Customer Feedback', 'string'),\n",
       " ('Smoking Status', 'int'),\n",
       " ('Exercise Frequency', 'string'),\n",
       " ('Property Type', 'string'),\n",
       " ('Premium Amount', 'double')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'Age',\n",
       " 'Gender',\n",
       " 'Annual Income',\n",
       " 'Marital Status',\n",
       " 'Number of Dependents',\n",
       " 'Education Level',\n",
       " 'Occupation',\n",
       " 'Health Score',\n",
       " 'Location',\n",
       " 'Policy Type',\n",
       " 'Previous Claims',\n",
       " 'Vehicle Age',\n",
       " 'Credit Score',\n",
       " 'Insurance Duration',\n",
       " 'Customer Feedback',\n",
       " 'Smoking Status',\n",
       " 'Exercise Frequency',\n",
       " 'Property Type',\n",
       " 'Premium Amount']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-------------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|  Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-------------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|      Premium|            2.0|       17.0|       372.0|               5.0|             Poor|             0|            Weekly|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|Comprehensive|            1.0|       12.0|       694.0|               2.0|          Average|             1|           Monthly|        House|        1483.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|      Premium|            1.0|       14.0|      592.92|               3.0|             Good|             1|            Weekly|        House|         567.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|        Basic|            1.0|        0.0|       367.0|               1.0|             Poor|             1|             Daily|    Apartment|         765.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|      Premium|            0.0|        8.0|       598.0|               4.0|             Poor|             1|            Weekly|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|      Premium|            2.0|        4.0|       614.0|               5.0|          Average|             0|            Weekly|        House|        3202.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|             25.61|   Rural|        Basic|            2.0|        8.0|       807.0|               6.0|             Poor|             0|            Weekly|        House|         439.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|Comprehensive|            1.0|       11.0|       398.0|               5.0|          Average|             0|            Rarely|        Condo|         111.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|      Premium|            1.0|       10.0|       685.0|               8.0|          Average|             0|           Monthly|        Condo|         213.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|Comprehensive|            1.0|        9.0|       635.0|               3.0|             Poor|             0|             Daily|        Condo|          64.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-------------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-------------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|  Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-------------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|      Premium|            2.0|       17.0|       372.0|               5.0|                1|             0|            Weekly|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|Comprehensive|            1.0|       12.0|       694.0|               2.0|                2|             1|           Monthly|        House|        1483.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|      Premium|            1.0|       14.0|      592.92|               3.0|                3|             1|            Weekly|        House|         567.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|        Basic|            1.0|        0.0|       367.0|               1.0|                1|             1|             Daily|    Apartment|         765.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|      Premium|            0.0|        8.0|       598.0|               4.0|                1|             1|            Weekly|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|      Premium|            2.0|        4.0|       614.0|               5.0|                2|             0|            Weekly|        House|        3202.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|             25.61|   Rural|        Basic|            2.0|        8.0|       807.0|               6.0|                1|             0|            Weekly|        House|         439.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|Comprehensive|            1.0|       11.0|       398.0|               5.0|                2|             0|            Rarely|        Condo|         111.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|      Premium|            1.0|       10.0|       685.0|               8.0|                2|             0|           Monthly|        Condo|         213.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|Comprehensive|            1.0|        9.0|       635.0|               3.0|                1|             0|             Daily|        Condo|          64.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-------------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn(\"Customer Feedback\", when(df2[\"Customer Feedback\"] == \"Poor\", 1)\n",
    "                                      .when(df2[\"Customer Feedback\"] == \"Average\", 2)\n",
    "                                      .when(df2[\"Customer Feedback\"] == \"Good\", 3)\n",
    "                                      .otherwise(0)\n",
    "                                      ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|            Weekly|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|           Monthly|        House|        1483.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|            Weekly|        House|         567.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|             Daily|    Apartment|         765.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|            Weekly|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|            Weekly|        House|        3202.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|             25.61|   Rural|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|            Weekly|        House|         439.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|            Rarely|        Condo|         111.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|           Monthly|        Condo|         213.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|             Daily|        Condo|          64.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn(\"Policy Type\", when(df2[\"Policy Type\"] == \"Basic\", 1)\n",
    "                                      .when(df2[\"Policy Type\"] == \"Comprehensive\", 2)\n",
    "                                      .when(df2[\"Policy Type\"] == \"Premium\", 3)\n",
    "                                      .otherwise(0)\n",
    "                                      ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|                 3|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|                 2|        House|        1483.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|                 3|        House|         567.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|                 4|    Apartment|         765.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|                 3|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|                 3|        House|        3202.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|             25.61|   Rural|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|                 3|        House|         439.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|                 1|        Condo|         111.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|                 2|        Condo|         213.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|                 4|        Condo|          64.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_e = 'Exercise Frequency'\n",
    "df2 = df2.withColumn(col_e, when(df2[col_e] == \"Daily\", 4)\n",
    "                                      .when(df2[col_e] == \"Weekly\", 3)\n",
    "                                      .when(df2[col_e] == \"Monthly\", 2)\n",
    "                                      .when(df2[col_e] == \"Rarely\", 1)\n",
    "                                      .otherwise(0)\n",
    "                                      ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------------+--------------+--------------------+---------------+----------+------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id|Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|Occupation|Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+---+------+-------------+--------------+--------------------+---------------+----------+------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|  0|     0|            0|             0|                   0|              0|         0|           0|       0|          0|         364029|          0|           0|                 0|                0|             0|                 0|            0|             0|\n",
      "+---+---+------+-------------+--------------+--------------------+---------------+----------+------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null values for each column\n",
    "null_counts = df2.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df2.columns])\n",
    "\n",
    "# Show the result\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_dropped = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_dropped=df2_dropped.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835971"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_dropped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.66425000000001"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df2_dropped.count()/df2.count())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------------+--------------+--------------------+---------------+----------+------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id|Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|Occupation|Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+---+------+-------------+--------------+--------------------+---------------+----------+------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|  0|     0|            0|             0|                   0|              0|         0|           0|       0|          0|              0|          0|           0|                 0|                0|             0|                 0|            0|             0|\n",
      "+---+---+------+-------------+--------------+--------------------+---------------+----------+------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "# total_nulls = df2_dropped.select([sum(when(col(c).isNull(), 1).otherwise(0)) for c in df2_dropped.columns]).collect()[0].asDict().values()\n",
    "# total_null_count = sum(total_nulls)\n",
    "\n",
    "# print(total_null_count)\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Count null values for each column\n",
    "null_counts = df2_dropped.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df2_dropped.columns])\n",
    "\n",
    "# Show the result\n",
    "null_counts.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df2_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|                 3|        House|        2869.0|           1.0|                   1.0|                    1.0|               1.0|             2.0|                  1.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|                 2|        House|        1483.0|           1.0|                   2.0|                    2.0|               0.0|             1.0|                  1.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|                 3|        House|         567.0|           0.0|                   2.0|                    3.0|               1.0|             0.0|                  1.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|                 4|    Apartment|         765.0|           0.0|                   1.0|                    1.0|               0.0|             1.0|                  0.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|                 3|        House|        2022.0|           0.0|                   0.0|                    1.0|               1.0|             1.0|                  1.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|                 3|        House|        3202.0|           0.0|                   1.0|                    1.0|               0.0|             2.0|                  1.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|             25.61|   Rural|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|                 3|        House|         439.0|           0.0|                   1.0|                    0.0|               0.0|             1.0|                  1.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|                 1|        Condo|         111.0|           1.0|                   2.0|                    3.0|               2.0|             0.0|                  2.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|                 2|        Condo|         213.0|           0.0|                   2.0|                    1.0|               0.0|             2.0|                  2.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|                 4|        Condo|          64.0|           0.0|                   1.0|                    2.0|               2.0|             2.0|                  2.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# string index\n",
    "#string indexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# to_string_indxn_columns = ['Gender', 'Marital Status', 'Education Level', 'Occupation',\n",
    "#                        'Location', 'Policy Type', 'Customer Feedback', \n",
    "#                        'Exercise Frequency', 'Property Type']\n",
    "\n",
    "\n",
    "to_string_indxn_columns = ['Gender', 'Marital Status', 'Education Level', \n",
    "                           'Occupation','Location',  'Property Type']\n",
    "\n",
    "# # index\n",
    "# indexers = [\n",
    "#     StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\").fit(df2)\n",
    "#     for col in to_string_indxn_columns\n",
    "# ]\n",
    "\n",
    "# # transform\n",
    "# df2 = indexers.transform(df2)\n",
    "\n",
    "\n",
    "# both index and transfmation together\n",
    "for col_name in to_string_indxn_columns:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_Indexed\" , handleInvalid=\"keep\")\n",
    "    df2_dropped = indexer.fit(df2_dropped).transform(df2_dropped)\n",
    "\n",
    "df2_dropped.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now drop the coulmns that we used to index new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "|  0|19.0|      10049.0|                 1.0| 22.59876067181393|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|                 3|        2869.0|           1.0|                   1.0|                    1.0|               1.0|             2.0|                  1.0|\n",
      "|  1|39.0|      31678.0|                 3.0|15.569730989408043|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|                 2|        1483.0|           1.0|                   2.0|                    2.0|               0.0|             1.0|                  1.0|\n",
      "|  2|23.0|      25602.0|                 3.0| 47.17754928786464|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|                 3|         567.0|           0.0|                   2.0|                    3.0|               1.0|             0.0|                  1.0|\n",
      "|  3|21.0|     141855.0|                 2.0|10.938144158664583|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|                 4|         765.0|           0.0|                   1.0|                    1.0|               0.0|             1.0|                  0.0|\n",
      "|  4|21.0|      39651.0|                 1.0|20.376093627736925|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|                 3|        2022.0|           0.0|                   0.0|                    1.0|               1.0|             1.0|                  1.0|\n",
      "|  5|29.0|      45963.0|                 1.0| 33.05319768402281|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|                 3|        3202.0|           0.0|                   1.0|                    1.0|               0.0|             2.0|                  1.0|\n",
      "|  6|41.0|      40336.0|                 0.0|             25.61|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|                 3|         439.0|           0.0|                   1.0|                    0.0|               0.0|             1.0|                  1.0|\n",
      "|  7|48.0|     127237.0|                 2.0| 5.769783092512088|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|                 1|         111.0|           1.0|                   2.0|                    3.0|               2.0|             0.0|                  2.0|\n",
      "|  8|21.0|       1733.0|                 3.0|17.869550814826297|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|                 2|         213.0|           0.0|                   2.0|                    1.0|               0.0|             2.0|                  2.0|\n",
      "|  9|44.0|      52447.0|                 2.0| 20.47371785695596|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|                 4|          64.0|           0.0|                   1.0|                    2.0|               2.0|             2.0|                  2.0|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed = df2_dropped.drop(*to_string_indxn_columns)\n",
    "df_indexed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training for ( Previous claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "|  0|19.0|      10049.0|                 1.0| 22.59876067181393|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|                 3|        2869.0|           1.0|                   1.0|                    1.0|               1.0|             2.0|                  1.0|\n",
      "|  1|39.0|      31678.0|                 3.0|15.569730989408043|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|                 2|        1483.0|           1.0|                   2.0|                    2.0|               0.0|             1.0|                  1.0|\n",
      "|  3|21.0|     141855.0|                 2.0|10.938144158664583|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|                 4|         765.0|           0.0|                   1.0|                    1.0|               0.0|             1.0|                  0.0|\n",
      "|  4|21.0|      39651.0|                 1.0|20.376093627736925|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|                 3|        2022.0|           0.0|                   0.0|                    1.0|               1.0|             1.0|                  1.0|\n",
      "|  5|29.0|      45963.0|                 1.0| 33.05319768402281|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|                 3|        3202.0|           0.0|                   1.0|                    1.0|               0.0|             2.0|                  1.0|\n",
      "|  7|48.0|     127237.0|                 2.0| 5.769783092512088|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|                 1|         111.0|           1.0|                   2.0|                    3.0|               2.0|             0.0|                  2.0|\n",
      "|  9|44.0|      52447.0|                 2.0| 20.47371785695596|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|                 4|          64.0|           0.0|                   1.0|                    2.0|               2.0|             2.0|                  2.0|\n",
      "| 10|56.0|       8054.0|                 1.0|             25.61|          3|            1.0|        8.0|       431.0|               8.0|                2|             0|                 2|         857.0|           1.0|                   1.0|                    1.0|               3.0|             1.0|                  2.0|\n",
      "| 12|25.0|      23706.0|                 4.0| 4.090538023921365|          2|            2.0|       19.0|      592.92|               2.0|                1|             1|                 3|         703.0|           1.0|                   0.0|                    2.0|               2.0|             2.0|                  0.0|\n",
      "| 13|44.0|      70893.0|                 0.0| 55.89632239159919|          3|            0.0|        3.0|       511.0|               6.0|                3|             1|                 3|        1847.0|           1.0|                   0.0|                    1.0|               0.0|             0.0|                  1.0|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting df2 into 80% training and 20% testing\n",
    "train_df, test_df = df_indexed.randomSplit([0.8, 0.2], seed=42)\n",
    "train_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "|  2|23.0|      25602.0|                 3.0| 47.17754928786464|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|                 3|         567.0|           0.0|                   2.0|                    3.0|               1.0|             0.0|                  1.0|\n",
      "|  6|41.0|      40336.0|                 0.0|             25.61|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|                 3|         439.0|           0.0|                   1.0|                    0.0|               0.0|             1.0|                  1.0|\n",
      "|  8|21.0|       1733.0|                 3.0|17.869550814826297|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|                 2|         213.0|           0.0|                   2.0|                    1.0|               0.0|             2.0|                  2.0|\n",
      "| 14|40.0|      23897.0|                 0.0| 29.08203615642357|          1|            2.0|       15.0|       498.0|               1.0|                3|             0|                 1|          30.0|           1.0|                   2.0|                    3.0|               1.0|             0.0|                  2.0|\n",
      "| 23|34.0|      32762.0|                 1.0| 21.96760938053775|          3|            0.0|       10.0|       798.0|               1.0|                2|             1|                 2|        3869.0|           1.0|                   2.0|                    1.0|               0.0|             1.0|                  2.0|\n",
      "| 31|37.0|       4534.0|                 0.0|26.387480918831336|          3|            0.0|        1.0|      592.92|               1.0|                1|             0|                 4|         165.0|           0.0|                   2.0|                    1.0|               0.0|             0.0|                  1.0|\n",
      "| 40|59.0|       1717.0|                 1.0|23.900048744874688|          1|            0.0|        9.0|       382.0|               6.0|                3|             0|                 3|        1813.0|           0.0|                   2.0|                    1.0|               3.0|             2.0|                  1.0|\n",
      "| 46|31.0|      27950.0|                 2.0| 22.93686152514115|          1|            0.0|        1.0|       575.0|               5.0|                1|             0|                 3|        1069.0|           1.0|                   1.0|                    0.0|               0.0|             2.0|                  1.0|\n",
      "| 63|29.0|       2490.0|                 0.0| 50.63801697762423|          2|            3.0|        7.0|      592.92|               2.0|                3|             0|                 2|         810.0|           1.0|                   1.0|                    2.0|               0.0|             2.0|                  0.0|\n",
      "| 64|48.0|      94904.0|                 1.0|  45.9792179435217|          2|            0.0|        3.0|       426.0|               1.0|                2|             0|                 2|         861.0|           1.0|                   2.0|                    1.0|               1.0|             2.0|                  0.0|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'Age',\n",
       " 'Annual Income',\n",
       " 'Number of Dependents',\n",
       " 'Health Score',\n",
       " 'Policy Type',\n",
       " 'Previous Claims',\n",
       " 'Vehicle Age',\n",
       " 'Credit Score',\n",
       " 'Insurance Duration',\n",
       " 'Customer Feedback',\n",
       " 'Smoking Status',\n",
       " 'Exercise Frequency',\n",
       " 'Premium Amount',\n",
       " 'Gender_Indexed',\n",
       " 'Marital Status_Indexed',\n",
       " 'Education Level_Indexed',\n",
       " 'Occupation_Indexed',\n",
       " 'Location_Indexed',\n",
       " 'Property Type_Indexed']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = ['id',\n",
    " 'Age',\n",
    " 'Annual Income',\n",
    " 'Number of Dependents',\n",
    " 'Health Score',\n",
    " 'Policy Type',\n",
    " 'Vehicle Age',\n",
    " 'Credit Score',\n",
    " 'Insurance Duration',\n",
    " 'Customer Feedback',\n",
    " 'Smoking Status',\n",
    " 'Exercise Frequency',\n",
    " 'Premium Amount',\n",
    " 'Gender_Indexed',\n",
    " 'Marital Status_Indexed',\n",
    " 'Education Level_Indexed',\n",
    " 'Occupation_Indexed',\n",
    " 'Location_Indexed',\n",
    " 'Property Type_Indexed']\n",
    "\n",
    "label_col = 'Previous Claims'\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorAssembler_c69b4cb9f906\n"
     ]
    }
   ],
   "source": [
    "print(assembler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform using assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = assembler.transform(train_df)\n",
    "test_data = assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|            features|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+\n",
      "|  0|19.0|      10049.0|                 1.0| 22.59876067181393|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|                 3|        2869.0|           1.0|                   1.0|                    1.0|               1.0|             2.0|                  1.0|[0.0,19.0,10049.0...|\n",
      "|  1|39.0|      31678.0|                 3.0|15.569730989408043|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|                 2|        1483.0|           1.0|                   2.0|                    2.0|               0.0|             1.0|                  1.0|[1.0,39.0,31678.0...|\n",
      "|  3|21.0|     141855.0|                 2.0|10.938144158664583|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|                 4|         765.0|           0.0|                   1.0|                    1.0|               0.0|             1.0|                  0.0|[3.0,21.0,141855....|\n",
      "|  4|21.0|      39651.0|                 1.0|20.376093627736925|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|                 3|        2022.0|           0.0|                   0.0|                    1.0|               1.0|             1.0|                  1.0|[4.0,21.0,39651.0...|\n",
      "|  5|29.0|      45963.0|                 1.0| 33.05319768402281|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|                 3|        3202.0|           0.0|                   1.0|                    1.0|               0.0|             2.0|                  1.0|[5.0,29.0,45963.0...|\n",
      "|  7|48.0|     127237.0|                 2.0| 5.769783092512088|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|                 1|         111.0|           1.0|                   2.0|                    3.0|               2.0|             0.0|                  2.0|[7.0,48.0,127237....|\n",
      "|  9|44.0|      52447.0|                 2.0| 20.47371785695596|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|                 4|          64.0|           0.0|                   1.0|                    2.0|               2.0|             2.0|                  2.0|[9.0,44.0,52447.0...|\n",
      "| 10|56.0|       8054.0|                 1.0|             25.61|          3|            1.0|        8.0|       431.0|               8.0|                2|             0|                 2|         857.0|           1.0|                   1.0|                    1.0|               3.0|             1.0|                  2.0|[10.0,56.0,8054.0...|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\",labelCol=label_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|            features|       rawPrediction|         probability|prediction|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  2|23.0|      25602.0|                 3.0| 47.17754928786464|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|                 3|         567.0|           0.0|                   2.0|                    3.0|               1.0|             0.0|                  1.0|[2.0,23.0,25602.0...|[7.69623989296887...|[0.38481199464844...|       0.0|\n",
      "|  6|41.0|      40336.0|                 0.0|             25.61|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|                 3|         439.0|           0.0|                   1.0|                    0.0|               0.0|             1.0|                  1.0|[6.0,41.0,40336.0...|[7.6121208364466,...|[0.38060604182232...|       0.0|\n",
      "|  8|21.0|       1733.0|                 3.0|17.869550814826297|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|                 2|         213.0|           0.0|                   2.0|                    1.0|               0.0|             2.0|                  2.0|[8.0,21.0,1733.0,...|[6.36644393971925...|[0.31832219698596...|       0.0|\n",
      "| 14|40.0|      23897.0|                 0.0| 29.08203615642357|          1|            2.0|       15.0|       498.0|               1.0|                3|             0|                 1|          30.0|           1.0|                   2.0|                    3.0|               1.0|             0.0|                  2.0|[14.0,40.0,23897....|[7.96378934128523...|[0.39818946706426...|       0.0|\n",
      "| 23|34.0|      32762.0|                 1.0| 21.96760938053775|          3|            0.0|       10.0|       798.0|               1.0|                2|             1|                 2|        3869.0|           1.0|                   2.0|                    1.0|               0.0|             1.0|                  2.0|[23.0,34.0,32762....|[7.25851114704168...|[0.36292555735208...|       0.0|\n",
      "| 31|37.0|       4534.0|                 0.0|26.387480918831336|          3|            0.0|        1.0|      592.92|               1.0|                1|             0|                 4|         165.0|           0.0|                   2.0|                    1.0|               0.0|             0.0|                  1.0|[31.0,37.0,4534.0...|[6.69991942870084...|[0.33499597143504...|       0.0|\n",
      "| 40|59.0|       1717.0|                 1.0|23.900048744874688|          1|            0.0|        9.0|       382.0|               6.0|                3|             0|                 3|        1813.0|           0.0|                   2.0|                    1.0|               3.0|             2.0|                  1.0|[40.0,59.0,1717.0...|[7.57693893650377...|[0.37884694682518...|       0.0|\n",
      "| 46|31.0|      27950.0|                 2.0| 22.93686152514115|          1|            0.0|        1.0|       575.0|               5.0|                1|             0|                 3|        1069.0|           1.0|                   1.0|                    0.0|               0.0|             2.0|                  1.0|[46.0,31.0,27950....|[7.67360197716560...|[0.38368009885828...|       0.0|\n",
      "| 63|29.0|       2490.0|                 0.0| 50.63801697762423|          2|            3.0|        7.0|      592.92|               2.0|                3|             0|                 2|         810.0|           1.0|                   1.0|                    2.0|               0.0|             2.0|                  0.0|[63.0,29.0,2490.0...|[5.82349149543793...|[0.29117457477189...|       2.0|\n",
      "| 64|48.0|      94904.0|                 1.0|  45.9792179435217|          2|            0.0|        3.0|       426.0|               1.0|                2|             0|                 2|         861.0|           1.0|                   2.0|                    1.0|               1.0|             2.0|                  0.0|[64.0,48.0,94904....|[7.64098080824145...|[0.38204904041207...|       0.0|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy using multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for random foest with multicalss classifier : 39.22%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize the evaluator\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Accuracy for random foest with multicalss classifier : {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling (previous claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**df2** is where  we'll apply the model to fill (previous claims) but :\n",
    "> ok here's the problem:\n",
    "df2 is main with null (previous claims)\n",
    "df2.dropped > is where i dropped all null (previous claims)\n",
    "df2.indexed > is where i string indexed all string columns , after i dropped the mother string columns and kept theri indexed columns\n",
    "how will i do it.?\n",
    "\n",
    "> i used **df2.indexed**\n",
    " too train the model. now my question is that for the **df2** how will we fill the null data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you trained the model on `df2.indexed`, you need to apply the same transformations to `df2` before making predictions. Here's how:  \n",
    "\n",
    "1. **Filter Rows with Null `previous claims`**  \n",
    "   - Extract rows from `df2` where `previous claims` is null.  \n",
    "\n",
    "2. **Apply Same String Indexing**  \n",
    "   - Ensure all categorical columns in `df2` are string-indexed using the same indexer from `df2.indexed`.  \n",
    "   - Drop original string columns, keeping only indexed ones (just like in `df2.indexed`).  \n",
    "\n",
    "3. **Make Predictions**  \n",
    "   - Use the trained Random Forest model to predict `previous claims` for these rows.  \n",
    "\n",
    "4. **Update `df2`**  \n",
    "   - Replace null values in `df2[\"previous claims\"]` with the predicted values.  \n",
    "\n",
    "Now, `df2` is fully filled with no missing `previous claims`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|                 3|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|                 2|        House|        1483.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|                 3|        House|         567.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|                 4|    Apartment|         765.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|                 3|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|                 3|        House|        3202.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|             25.61|   Rural|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|                 3|        House|         439.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|                 1|        Condo|         111.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|                 2|        Condo|         213.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|                 4|        Condo|          64.0|\n",
      "| 10|56.0|Female|       8054.0|       Married|                 1.0|     Bachelor's|        Unemployed|             25.61|   Rural|          3|            1.0|        8.0|       431.0|               8.0|                2|             0|                 2|        Condo|         857.0|\n",
      "| 11|23.0|  Male|      30983.0|        Single|                 3.0|       Master's|Unknown_Occupation| 5.813128940949042|   Urban|          3|           NULL|        6.0|       597.0|               8.0|                3|             0|                 1|        Condo|        1447.0|\n",
      "| 12|25.0|Female|      23706.0|        Single|                 4.0|       Master's|          Employed| 4.090538023921365|   Urban|          2|            2.0|       19.0|      592.92|               2.0|                1|             1|                 3|    Apartment|         703.0|\n",
      "| 13|44.0|Female|      70893.0|        Single|                 0.0|     Bachelor's|Unknown_Occupation| 55.89632239159919|Suburban|          3|            0.0|        3.0|       511.0|               6.0|                3|             1|                 3|        House|        1847.0|\n",
      "| 14|40.0|Female|      23897.0|      Divorced|                 0.0|    High School|     Self-Employed| 29.08203615642357|Suburban|          1|            2.0|       15.0|       498.0|               1.0|                3|             0|                 1|        Condo|          30.0|\n",
      "| 15|18.0|  Male|       6076.0|       Married|                 2.0|    High School|          Employed| 7.442964015746718|   Urban|          3|            1.0|       12.0|       584.0|               5.0|                3|             1|                 2|    Apartment|         849.0|\n",
      "| 16|59.0|Female|      28266.0|      Divorced|                 2.0|            PhD|        Unemployed| 21.67346054211263|   Urban|          1|            0.0|       16.0|      592.92|               3.0|                2|             1|                 3|        Condo|         183.0|\n",
      "| 17|34.0|Female|      45907.0|      Divorced|                 4.0|    High School|     Self-Employed|  24.0596387644636|Suburban|          2|            0.0|        0.0|       694.0|               8.0|                1|             1|                 4|        Condo|         643.0|\n",
      "| 18|18.0|  Male|      29071.0|       Married|                 0.0|     Bachelor's|          Employed|  20.8389765352726|   Rural|          3|            1.0|        5.0|      592.92|               3.0|                1|             1|                 1|    Apartment|         787.0|\n",
      "| 19|40.0|Female|     123751.0|        Single|                 2.0|       Master's|     Self-Employed| 24.95531647911226|Suburban|          3|            0.0|        8.0|       420.0|               2.0|                3|             1|                 1|        Condo|          40.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| 11|23.0|  Male|      30983.0|        Single|                 3.0|       Master's|Unknown_Occupation| 5.813128940949042|   Urban|          3|           NULL|        6.0|       597.0|               8.0|                3|             0|                 1|        Condo|        1447.0|\n",
      "| 20|39.0|  Male|       6837.0|      Divorced|                 2.0|    High School|     Self-Employed|17.814458917250235|   Urban|          3|           NULL|        7.0|       595.0|               2.0|                3|             1|                 4|    Apartment|        1204.0|\n",
      "| 21|44.0|Female|      14042.0|      Divorced|                 2.0|       Master's|Unknown_Occupation| 4.551825118494738|   Urban|          1|           NULL|        6.0|       799.0|               5.0|                3|             1|                 3|    Apartment|        2670.0|\n",
      "| 22|22.0|  Male|     32745.22|      Divorced|                 4.0|            PhD|Unknown_Occupation| 25.58378995160535|   Urban|          2|           NULL|        5.0|       773.0|               5.0|                3|             1|                 2|        House|         202.0|\n",
      "| 24|46.0|  Male|      24708.0|       Married|                 3.0|    High School|          Employed|  26.3412591810326|   Urban|          3|           NULL|        0.0|       543.0|               4.0|                2|             0|                 4|        House|        1136.0|\n",
      "| 26|49.0|Female|      82584.0|       Married|                 0.0|     Bachelor's|        Unemployed| 23.86738827838687|   Rural|          2|           NULL|       10.0|       425.0|               9.0|                2|             1|                 3|    Apartment|        1010.0|\n",
      "| 29|64.0|  Male|     131038.0|        Single|                 0.0|            PhD|     Self-Employed| 20.71707646913313|   Rural|          3|           NULL|       14.0|       495.0|               1.0|                3|             1|                 2|        House|        2360.0|\n",
      "| 30|52.0|  Male|      30350.0|        Single|                 1.0|     Bachelor's|          Employed| 21.49661354547669|   Rural|          2|           NULL|       18.0|       449.0|               1.0|                1|             0|                 1|        House|         641.0|\n",
      "| 33|40.0|Female|       1190.0|        Single|                 2.0|    High School|          Employed| 47.85157218465031|   Rural|          1|           NULL|        0.0|       421.0|               4.0|                3|             0|                 3|        Condo|        1128.0|\n",
      "| 34|25.0|Female|      40887.0|        Single|                 3.0|     Bachelor's|Unknown_Occupation|17.283651290198883|Suburban|          3|           NULL|        5.0|       695.0|               2.0|                2|             1|                 2|    Apartment|        2152.0|\n",
      "| 38|50.0|Female|      57423.0|        Single|                 1.0|     Bachelor's|     Self-Employed| 14.14668411986696|   Urban|          1|           NULL|        6.0|      592.92|               3.0|                2|             0|                 4|        Condo|        3890.0|\n",
      "| 50|30.0|Female|       8599.0|        Single|                 1.0|            PhD|          Employed|28.744667545434645|   Rural|          2|           NULL|       18.0|       784.0|               7.0|                3|             0|                 1|        House|         882.0|\n",
      "| 52|33.0|  Male|      54962.0|       Married|                 0.0|       Master's|          Employed| 20.08451192668927|   Urban|          3|           NULL|        9.0|       381.0|               9.0|                2|             0|                 4|        House|        1358.0|\n",
      "| 53|50.0|Female|       4928.0|      Divorced|                 1.0|            PhD|        Unemployed|  17.8827123747686|Suburban|          2|           NULL|        3.0|       801.0|               7.0|                3|             1|                 3|    Apartment|        2312.0|\n",
      "| 57|58.0|  Male|      78986.0|      Divorced|                 2.0|            PhD|     Self-Employed|22.692009207964325|   Urban|          3|           NULL|       16.0|       497.0|               9.0|                3|             0|                 2|    Apartment|          78.0|\n",
      "| 58|50.0|Female|      35353.0|      Divorced|                 1.0|     Bachelor's|          Employed|35.619122195892686|Suburban|          3|           NULL|        5.0|       359.0|               3.0|                2|             1|                 1|        House|         745.0|\n",
      "| 59|62.0|Female|     147020.0|       Married|                 4.0|     Bachelor's|     Self-Employed| 24.12174594769064|   Urban|          2|           NULL|        0.0|       498.0|               6.0|                0|             0|                 4|        Condo|        1624.0|\n",
      "| 61|40.0|Female|      11273.0|      Divorced|                 3.0|    High School|          Employed|  51.5065330307197|   Rural|          2|           NULL|       19.0|       703.0|               5.0|                2|             1|                 2|        Condo|         562.0|\n",
      "| 67|45.0|  Male|     32745.22|       Married|                 3.0|    High School|     Self-Employed|17.988631364333468|Suburban|          2|           NULL|       18.0|       375.0|               9.0|                2|             1|                 1|        House|         829.0|\n",
      "| 79|27.0|  Male|        948.0|        Single|                 3.0|       Master's|Unknown_Occupation| 37.47954132171145|   Rural|          1|           NULL|       16.0|       534.0|               9.0|                0|             0|                 1|        House|         514.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Identify missing values\n",
    "missing_df = df2.filter(col(\"previous claims\").isNull())\n",
    "missing_df.show()\n",
    "#missing_data = df2.filter(col(\"Previous Claims\").isNull()).drop(\"Previous Claims\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364029"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| 11|23.0|  Male|      30983.0|        Single|                 3.0|       Master's|Unknown_Occupation| 5.813128940949042|   Urban|          3|           NULL|        6.0|       597.0|               8.0|                3|             0|                 1|        Condo|        1447.0|           0.0|                   1.0|                    0.0|               0.0|             2.0|                  1.0|\n",
      "| 20|39.0|  Male|       6837.0|      Divorced|                 2.0|    High School|     Self-Employed|17.814458917250235|   Urban|          3|           NULL|        7.0|       595.0|               2.0|                3|             1|                 4|    Apartment|        1204.0|           0.0|                   2.0|                    3.0|               2.0|             2.0|                  2.0|\n",
      "| 21|44.0|Female|      14042.0|      Divorced|                 2.0|       Master's|Unknown_Occupation| 4.551825118494738|   Urban|          1|           NULL|        6.0|       799.0|               5.0|                3|             1|                 3|    Apartment|        2670.0|           1.0|                   2.0|                    0.0|               0.0|             2.0|                  2.0|\n",
      "| 22|22.0|  Male|     32745.22|      Divorced|                 4.0|            PhD|Unknown_Occupation| 25.58378995160535|   Urban|          2|           NULL|        5.0|       773.0|               5.0|                3|             1|                 2|        House|         202.0|           0.0|                   2.0|                    2.0|               0.0|             2.0|                  0.0|\n",
      "| 24|46.0|  Male|      24708.0|       Married|                 3.0|    High School|          Employed|  26.3412591810326|   Urban|          3|           NULL|        0.0|       543.0|               4.0|                2|             0|                 4|        House|        1136.0|           0.0|                   0.0|                    3.0|               1.0|             2.0|                  0.0|\n",
      "| 26|49.0|Female|      82584.0|       Married|                 0.0|     Bachelor's|        Unemployed| 23.86738827838687|   Rural|          2|           NULL|       10.0|       425.0|               9.0|                2|             1|                 3|    Apartment|        1010.0|           1.0|                   0.0|                    1.0|               3.0|             0.0|                  2.0|\n",
      "| 29|64.0|  Male|     131038.0|        Single|                 0.0|            PhD|     Self-Employed| 20.71707646913313|   Rural|          3|           NULL|       14.0|       495.0|               1.0|                3|             1|                 2|        House|        2360.0|           0.0|                   1.0|                    2.0|               2.0|             0.0|                  0.0|\n",
      "| 30|52.0|  Male|      30350.0|        Single|                 1.0|     Bachelor's|          Employed| 21.49661354547669|   Rural|          2|           NULL|       18.0|       449.0|               1.0|                1|             0|                 1|        House|         641.0|           0.0|                   1.0|                    1.0|               1.0|             0.0|                  0.0|\n",
      "| 33|40.0|Female|       1190.0|        Single|                 2.0|    High School|          Employed| 47.85157218465031|   Rural|          1|           NULL|        0.0|       421.0|               4.0|                3|             0|                 3|        Condo|        1128.0|           1.0|                   1.0|                    3.0|               1.0|             0.0|                  1.0|\n",
      "| 34|25.0|Female|      40887.0|        Single|                 3.0|     Bachelor's|Unknown_Occupation|17.283651290198883|Suburban|          3|           NULL|        5.0|       695.0|               2.0|                2|             1|                 2|    Apartment|        2152.0|           1.0|                   1.0|                    1.0|               0.0|             1.0|                  2.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "to_string_indxn_columns = ['Gender', 'Marital Status', 'Education Level', \n",
    "                           'Occupation','Location',  'Property Type']\n",
    "\n",
    "# both index and transfmation together\n",
    "for col_name in to_string_indxn_columns:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_Indexed\" , handleInvalid=\"keep\")\n",
    "    missing_df = indexer.fit(missing_df).transform(missing_df)\n",
    "\n",
    "missing_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| 11|23.0|      30983.0|                 3.0| 5.813128940949042|          3|           NULL|        6.0|       597.0|               8.0|                3|             0|                 1|        1447.0|           0.0|                   1.0|                    0.0|               0.0|             2.0|                  1.0|\n",
      "| 20|39.0|       6837.0|                 2.0|17.814458917250235|          3|           NULL|        7.0|       595.0|               2.0|                3|             1|                 4|        1204.0|           0.0|                   2.0|                    3.0|               2.0|             2.0|                  2.0|\n",
      "| 21|44.0|      14042.0|                 2.0| 4.551825118494738|          1|           NULL|        6.0|       799.0|               5.0|                3|             1|                 3|        2670.0|           1.0|                   2.0|                    0.0|               0.0|             2.0|                  2.0|\n",
      "| 22|22.0|     32745.22|                 4.0| 25.58378995160535|          2|           NULL|        5.0|       773.0|               5.0|                3|             1|                 2|         202.0|           0.0|                   2.0|                    2.0|               0.0|             2.0|                  0.0|\n",
      "| 24|46.0|      24708.0|                 3.0|  26.3412591810326|          3|           NULL|        0.0|       543.0|               4.0|                2|             0|                 4|        1136.0|           0.0|                   0.0|                    3.0|               1.0|             2.0|                  0.0|\n",
      "| 26|49.0|      82584.0|                 0.0| 23.86738827838687|          2|           NULL|       10.0|       425.0|               9.0|                2|             1|                 3|        1010.0|           1.0|                   0.0|                    1.0|               3.0|             0.0|                  2.0|\n",
      "| 29|64.0|     131038.0|                 0.0| 20.71707646913313|          3|           NULL|       14.0|       495.0|               1.0|                3|             1|                 2|        2360.0|           0.0|                   1.0|                    2.0|               2.0|             0.0|                  0.0|\n",
      "| 30|52.0|      30350.0|                 1.0| 21.49661354547669|          2|           NULL|       18.0|       449.0|               1.0|                1|             0|                 1|         641.0|           0.0|                   1.0|                    1.0|               1.0|             0.0|                  0.0|\n",
      "| 33|40.0|       1190.0|                 2.0| 47.85157218465031|          1|           NULL|        0.0|       421.0|               4.0|                3|             0|                 3|        1128.0|           1.0|                   1.0|                    3.0|               1.0|             0.0|                  1.0|\n",
      "| 34|25.0|      40887.0|                 3.0|17.283651290198883|          3|           NULL|        5.0|       695.0|               2.0|                2|             1|                 2|        2152.0|           1.0|                   1.0|                    1.0|               0.0|             1.0|                  2.0|\n",
      "+---+----+-------------+--------------------+------------------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_df_indexed = missing_df.drop(*to_string_indxn_columns)\n",
    "missing_df_indexed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+-----------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Policy Type|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|\n",
      "+---+----+-------------+--------------------+------------------+-----------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| 11|23.0|      30983.0|                 3.0| 5.813128940949042|          3|        6.0|       597.0|               8.0|                3|             0|                 1|        1447.0|           0.0|                   1.0|                    0.0|               0.0|             2.0|                  1.0|\n",
      "| 20|39.0|       6837.0|                 2.0|17.814458917250235|          3|        7.0|       595.0|               2.0|                3|             1|                 4|        1204.0|           0.0|                   2.0|                    3.0|               2.0|             2.0|                  2.0|\n",
      "| 21|44.0|      14042.0|                 2.0| 4.551825118494738|          1|        6.0|       799.0|               5.0|                3|             1|                 3|        2670.0|           1.0|                   2.0|                    0.0|               0.0|             2.0|                  2.0|\n",
      "| 22|22.0|     32745.22|                 4.0| 25.58378995160535|          2|        5.0|       773.0|               5.0|                3|             1|                 2|         202.0|           0.0|                   2.0|                    2.0|               0.0|             2.0|                  0.0|\n",
      "| 24|46.0|      24708.0|                 3.0|  26.3412591810326|          3|        0.0|       543.0|               4.0|                2|             0|                 4|        1136.0|           0.0|                   0.0|                    3.0|               1.0|             2.0|                  0.0|\n",
      "+---+----+-------------+--------------------+------------------+-----------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_data = missing_df_indexed.drop(\"Previous Claims\")\n",
    "missing_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Features extract koro (same as training time)\n",
    "missing_data = assembler.transform(missing_data)  # Ensure same preprocessing steps\n",
    "\n",
    "# Step 3: Prediction koro\n",
    "missing_predictions = rf_model.transform(missing_data).select(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364029"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+-----------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+----------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Policy Type|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|            features|prediction|\n",
      "+---+----+-------------+--------------------+------------------+-----------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+----------+\n",
      "| 21|44.0|      14042.0|                 2.0| 4.551825118494738|          1|        6.0|       799.0|               5.0|                3|             1|                 3|        2670.0|           1.0|                   2.0|                    0.0|               0.0|             2.0|                  2.0|[21.0,44.0,14042....|       0.0|\n",
      "| 24|46.0|      24708.0|                 3.0|  26.3412591810326|          3|        0.0|       543.0|               4.0|                2|             0|                 4|        1136.0|           0.0|                   0.0|                    3.0|               1.0|             2.0|                  0.0|[24.0,46.0,24708....|       0.0|\n",
      "| 26|49.0|      82584.0|                 0.0| 23.86738827838687|          2|       10.0|       425.0|               9.0|                2|             1|                 3|        1010.0|           1.0|                   0.0|                    1.0|               3.0|             0.0|                  2.0|[26.0,49.0,82584....|       0.0|\n",
      "| 33|40.0|       1190.0|                 2.0| 47.85157218465031|          1|        0.0|       421.0|               4.0|                3|             0|                 3|        1128.0|           1.0|                   1.0|                    3.0|               1.0|             0.0|                  1.0|[33.0,40.0,1190.0...|       0.0|\n",
      "| 52|33.0|      54962.0|                 0.0| 20.08451192668927|          3|        9.0|       381.0|               9.0|                2|             0|                 4|        1358.0|           0.0|                   0.0|                    0.0|               1.0|             2.0|                  0.0|[52.0,33.0,54962....|       0.0|\n",
      "| 58|50.0|      35353.0|                 1.0|35.619122195892686|          3|        5.0|       359.0|               3.0|                2|             1|                 1|         745.0|           1.0|                   2.0|                    1.0|               1.0|             1.0|                  0.0|[58.0,50.0,35353....|       0.0|\n",
      "| 79|27.0|        948.0|                 3.0| 37.47954132171145|          1|       16.0|       534.0|               9.0|                0|             0|                 1|         514.0|           0.0|                   1.0|                    0.0|               0.0|             0.0|                  0.0|[79.0,27.0,948.0,...|       0.0|\n",
      "| 86|37.0|     32745.22|                 1.0|  13.0382686038979|          2|        6.0|       562.0|               2.0|                2|             0|                 3|          61.0|           0.0|                   1.0|                    1.0|               0.0|             1.0|                  0.0|[86.0,37.0,32745....|       0.0|\n",
      "| 93|33.0|      43974.0|                 0.0| 39.05648818673042|          1|       16.0|       638.0|               5.0|                2|             1|                 3|        3227.0|           0.0|                   2.0|                    1.0|               2.0|             1.0|                  2.0|[93.0,33.0,43974....|       0.0|\n",
      "| 94|21.0|      43481.0|                 3.0| 24.75443497976334|          2|       18.0|       486.0|               5.0|                0|             0|                 2|        1313.0|           1.0|                   2.0|                    0.0|               0.0|             0.0|                  2.0|[94.0,21.0,43481....|       0.0|\n",
      "+---+----+-------------+--------------------+------------------+-----------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure row order is maintained by using zipWithIndex\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Step 1: Add a unique index to both DataFrames\n",
    "missing_data = missing_data.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "missing_predictions = missing_predictions.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# Step 2: Join based on \"row_id\"\n",
    "missing_data_with_predictions = missing_data.join(missing_predictions, on=\"row_id\").drop(\"row_id\")\n",
    "\n",
    "# Check the result\n",
    "missing_data_with_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+-----------------+-----------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+----------+\n",
      "| id| Age|Annual Income|Number of Dependents|     Health Score|Policy Type|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|            features|prediction|\n",
      "+---+----+-------------+--------------------+-----------------+-----------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+----------+\n",
      "| 11|23.0|      30983.0|                 3.0|5.813128940949042|          3|        6.0|       597.0|               8.0|                3|             0|                 1|        1447.0|           0.0|                   1.0|                    0.0|               0.0|             2.0|                  1.0|[11.0,23.0,30983....|       0.0|\n",
      "+---+----+-------------+--------------------+-----------------+-----------+-----------+------------+------------------+-----------------+--------------+------------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = missing_data_with_predictions\n",
    "df.filter(col(\"id\") == 11).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|Gender_Indexed|Marital Status_Indexed|Education Level_Indexed|Occupation_Indexed|Location_Indexed|Property Type_Indexed|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|                 3|        House|        2869.0|           1.0|                   1.0|                    2.0|               2.0|             2.0|                  0.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|                 2|        House|        1483.0|           1.0|                   2.0|                    0.0|               0.0|             1.0|                  0.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|                 3|        House|         567.0|           0.0|                   2.0|                    3.0|               2.0|             0.0|                  0.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|                 4|    Apartment|         765.0|           0.0|                   1.0|                    2.0|               0.0|             1.0|                  1.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|                 3|        House|        2022.0|           0.0|                   0.0|                    2.0|               2.0|             1.0|                  0.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|                 3|        House|        3202.0|           0.0|                   1.0|                    2.0|               0.0|             2.0|                  0.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|             25.61|   Rural|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|                 3|        House|         439.0|           0.0|                   1.0|                    1.0|               0.0|             1.0|                  0.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|                 1|        Condo|         111.0|           1.0|                   2.0|                    3.0|               1.0|             0.0|                  2.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|                 2|        Condo|         213.0|           0.0|                   2.0|                    2.0|               0.0|             2.0|                  2.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|                 4|        Condo|          64.0|           0.0|                   1.0|                    0.0|               1.0|             2.0|                  2.0|\n",
      "| 10|56.0|Female|       8054.0|       Married|                 1.0|     Bachelor's|        Unemployed|             25.61|   Rural|          3|            1.0|        8.0|       431.0|               8.0|                2|             0|                 2|        Condo|         857.0|           1.0|                   1.0|                    2.0|               3.0|             1.0|                  2.0|\n",
      "| 11|23.0|  Male|      30983.0|        Single|                 3.0|       Master's|Unknown_Occupation| 5.813128940949042|   Urban|          3|           NULL|        6.0|       597.0|               8.0|                3|             0|                 1|        Condo|        1447.0|           0.0|                   0.0|                    0.0|               0.0|             2.0|                  2.0|\n",
      "| 12|25.0|Female|      23706.0|        Single|                 4.0|       Master's|          Employed| 4.090538023921365|   Urban|          2|            2.0|       19.0|      592.92|               2.0|                1|             1|                 3|    Apartment|         703.0|           1.0|                   0.0|                    0.0|               1.0|             2.0|                  1.0|\n",
      "| 13|44.0|Female|      70893.0|        Single|                 0.0|     Bachelor's|Unknown_Occupation| 55.89632239159919|Suburban|          3|            0.0|        3.0|       511.0|               6.0|                3|             1|                 3|        House|        1847.0|           1.0|                   0.0|                    2.0|               0.0|             0.0|                  0.0|\n",
      "| 14|40.0|Female|      23897.0|      Divorced|                 0.0|    High School|     Self-Employed| 29.08203615642357|Suburban|          1|            2.0|       15.0|       498.0|               1.0|                3|             0|                 1|        Condo|          30.0|           1.0|                   2.0|                    3.0|               2.0|             0.0|                  2.0|\n",
      "| 15|18.0|  Male|       6076.0|       Married|                 2.0|    High School|          Employed| 7.442964015746718|   Urban|          3|            1.0|       12.0|       584.0|               5.0|                3|             1|                 2|    Apartment|         849.0|           0.0|                   1.0|                    3.0|               1.0|             2.0|                  1.0|\n",
      "| 16|59.0|Female|      28266.0|      Divorced|                 2.0|            PhD|        Unemployed| 21.67346054211263|   Urban|          1|            0.0|       16.0|      592.92|               3.0|                2|             1|                 3|        Condo|         183.0|           1.0|                   2.0|                    1.0|               3.0|             2.0|                  2.0|\n",
      "| 17|34.0|Female|      45907.0|      Divorced|                 4.0|    High School|     Self-Employed|  24.0596387644636|Suburban|          2|            0.0|        0.0|       694.0|               8.0|                1|             1|                 4|        Condo|         643.0|           1.0|                   2.0|                    3.0|               2.0|             0.0|                  2.0|\n",
      "| 18|18.0|  Male|      29071.0|       Married|                 0.0|     Bachelor's|          Employed|  20.8389765352726|   Rural|          3|            1.0|        5.0|      592.92|               3.0|                1|             1|                 1|    Apartment|         787.0|           0.0|                   1.0|                    2.0|               1.0|             1.0|                  1.0|\n",
      "| 19|40.0|Female|     123751.0|        Single|                 2.0|       Master's|     Self-Employed| 24.95531647911226|Suburban|          3|            0.0|        8.0|       420.0|               2.0|                3|             1|                 1|        Condo|          40.0|           1.0|                   0.0|                    0.0|               2.0|             0.0|                  2.0|\n",
      "| 20|39.0|  Male|       6837.0|      Divorced|                 2.0|    High School|     Self-Employed|17.814458917250235|   Urban|          3|           NULL|        7.0|       595.0|               2.0|                3|             1|                 4|    Apartment|        1204.0|           0.0|                   2.0|                    3.0|               2.0|             2.0|                  1.0|\n",
      "| 21|44.0|Female|      14042.0|      Divorced|                 2.0|       Master's|Unknown_Occupation| 4.551825118494738|   Urban|          1|           NULL|        6.0|       799.0|               5.0|                3|             1|                 3|    Apartment|        2670.0|           1.0|                   2.0|                    0.0|               0.0|             2.0|                  1.0|\n",
      "| 22|22.0|  Male|     32745.22|      Divorced|                 4.0|            PhD|Unknown_Occupation| 25.58378995160535|   Urban|          2|           NULL|        5.0|       773.0|               5.0|                3|             1|                 2|        House|         202.0|           0.0|                   2.0|                    1.0|               0.0|             2.0|                  0.0|\n",
      "| 23|34.0|Female|      32762.0|      Divorced|                 1.0|     Bachelor's|Unknown_Occupation| 21.96760938053775|   Rural|          3|            0.0|       10.0|       798.0|               1.0|                2|             1|                 2|        Condo|        3869.0|           1.0|                   2.0|                    2.0|               0.0|             1.0|                  2.0|\n",
      "| 24|46.0|  Male|      24708.0|       Married|                 3.0|    High School|          Employed|  26.3412591810326|   Urban|          3|           NULL|        0.0|       543.0|               4.0|                2|             0|                 4|        House|        1136.0|           0.0|                   1.0|                    3.0|               1.0|             2.0|                  0.0|\n",
      "| 25|19.0|  Male|      72482.0|       Married|                 2.0|     Bachelor's|        Unemployed| 27.27941569575117|Suburban|          3|            0.0|       17.0|       416.0|               3.0|                3|             1|                 3|        Condo|         671.0|           0.0|                   1.0|                    2.0|               3.0|             0.0|                  2.0|\n",
      "| 26|49.0|Female|      82584.0|       Married|                 0.0|     Bachelor's|        Unemployed| 23.86738827838687|   Rural|          2|           NULL|       10.0|       425.0|               9.0|                2|             1|                 3|    Apartment|        1010.0|           1.0|                   1.0|                    2.0|               3.0|             1.0|                  1.0|\n",
      "| 27|42.0|  Male|      35123.0|       Married|                 2.0|     Bachelor's|     Self-Employed| 31.39190719557818|Suburban|          3|            0.0|        3.0|       486.0|               4.0|                3|             1|                 1|        Condo|        2680.0|           0.0|                   1.0|                    2.0|               2.0|             0.0|                  2.0|\n",
      "| 28|43.0|  Male|       1060.0|       Married|                 0.0|       Master's|     Self-Employed| 41.37671628604045|Suburban|          3|            0.0|        8.0|       795.0|               7.0|                0|             1|                 1|        Condo|         699.0|           0.0|                   1.0|                    0.0|               2.0|             0.0|                  2.0|\n",
      "| 29|64.0|  Male|     131038.0|        Single|                 0.0|            PhD|     Self-Employed| 20.71707646913313|   Rural|          3|           NULL|       14.0|       495.0|               1.0|                3|             1|                 2|        House|        2360.0|           0.0|                   0.0|                    1.0|               2.0|             1.0|                  0.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+--------------+----------------------+-----------------------+------------------+----------------+---------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "to_string_indxn_columns = ['Gender', 'Marital Status', 'Education Level', \n",
    "                           'Occupation','Location',  'Property Type']\n",
    "new_df = df2\n",
    "# both index and transfmation together\n",
    "for col_name in to_string_indxn_columns:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_Indexed\" , handleInvalid=\"keep\")\n",
    "    new_df = indexer.fit(new_df).transform(new_df)\n",
    "\n",
    "new_df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|                 3|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|                 2|        House|        1483.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|                 3|        House|         567.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|                 4|    Apartment|         765.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|                 3|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|                 3|        House|        3202.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|             25.61|   Rural|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|                 3|        House|         439.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|                 1|        Condo|         111.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|                 2|        Condo|         213.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|                 4|        Condo|          64.0|\n",
      "| 10|56.0|Female|       8054.0|       Married|                 1.0|     Bachelor's|        Unemployed|             25.61|   Rural|          3|            1.0|        8.0|       431.0|               8.0|                2|             0|                 2|        Condo|         857.0|\n",
      "| 11|23.0|  Male|      30983.0|        Single|                 3.0|       Master's|Unknown_Occupation| 5.813128940949042|   Urban|          3|           NULL|        6.0|       597.0|               8.0|                3|             0|                 1|        Condo|        1447.0|\n",
      "| 12|25.0|Female|      23706.0|        Single|                 4.0|       Master's|          Employed| 4.090538023921365|   Urban|          2|            2.0|       19.0|      592.92|               2.0|                1|             1|                 3|    Apartment|         703.0|\n",
      "| 13|44.0|Female|      70893.0|        Single|                 0.0|     Bachelor's|Unknown_Occupation| 55.89632239159919|Suburban|          3|            0.0|        3.0|       511.0|               6.0|                3|             1|                 3|        House|        1847.0|\n",
      "| 14|40.0|Female|      23897.0|      Divorced|                 0.0|    High School|     Self-Employed| 29.08203615642357|Suburban|          1|            2.0|       15.0|       498.0|               1.0|                3|             0|                 1|        Condo|          30.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_df = df2\n",
    "old_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|        Occupation|      Health Score|Location|Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "| 12|25.0|Female|      23706.0|        Single|                 4.0|       Master's|          Employed| 4.090538023921365|   Urban|          2|            2.0|       19.0|      592.92|               2.0|                1|             1|                 3|    Apartment|         703.0|\n",
      "| 13|44.0|Female|      70893.0|        Single|                 0.0|     Bachelor's|Unknown_Occupation| 55.89632239159919|Suburban|          3|            0.0|        3.0|       511.0|               6.0|                3|             1|                 3|        House|        1847.0|\n",
      "| 14|40.0|Female|      23897.0|      Divorced|                 0.0|    High School|     Self-Employed| 29.08203615642357|Suburban|          1|            2.0|       15.0|       498.0|               1.0|                3|             0|                 1|        Condo|          30.0|\n",
      "| 18|18.0|  Male|      29071.0|       Married|                 0.0|     Bachelor's|          Employed|  20.8389765352726|   Rural|          3|            1.0|        5.0|      592.92|               3.0|                1|             1|                 1|    Apartment|         787.0|\n",
      "|  6|41.0|  Male|      40336.0|       Married|                 0.0|            PhD|Unknown_Occupation|             25.61|   Rural|          1|            2.0|        8.0|       807.0|               6.0|                1|             0|                 3|        House|         439.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|          Employed| 20.47371785695596|   Urban|          2|            1.0|        9.0|       635.0|               3.0|                1|             0|                 4|        Condo|          64.0|\n",
      "| 15|18.0|  Male|       6076.0|       Married|                 2.0|    High School|          Employed| 7.442964015746718|   Urban|          3|            1.0|       12.0|       584.0|               5.0|                3|             1|                 2|    Apartment|         849.0|\n",
      "| 16|59.0|Female|      28266.0|      Divorced|                 2.0|            PhD|        Unemployed| 21.67346054211263|   Urban|          1|            0.0|       16.0|      592.92|               3.0|                2|             1|                 3|        Condo|         183.0|\n",
      "| 17|34.0|Female|      45907.0|      Divorced|                 4.0|    High School|     Self-Employed|  24.0596387644636|Suburban|          2|            0.0|        0.0|       694.0|               8.0|                1|             1|                 4|        Condo|         643.0|\n",
      "| 19|40.0|Female|     123751.0|        Single|                 2.0|       Master's|     Self-Employed| 24.95531647911226|Suburban|          3|            0.0|        8.0|       420.0|               2.0|                3|             1|                 1|        Condo|          40.0|\n",
      "| 20|39.0|  Male|       6837.0|      Divorced|                 2.0|    High School|     Self-Employed|17.814458917250235|   Urban|          3|            0.0|        7.0|       595.0|               2.0|                3|             1|                 4|    Apartment|        1204.0|\n",
      "|  2|23.0|  Male|      25602.0|      Divorced|                 3.0|    High School|     Self-Employed| 47.17754928786464|Suburban|          3|            1.0|       14.0|      592.92|               3.0|                3|             1|                 3|        House|         567.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|     Self-Employed|20.376093627736925|   Rural|          3|            0.0|        8.0|       598.0|               4.0|                1|             1|                 3|        House|        2022.0|\n",
      "|  5|29.0|  Male|      45963.0|       Married|                 1.0|     Bachelor's|Unknown_Occupation| 33.05319768402281|   Urban|          3|            2.0|        4.0|       614.0|               5.0|                2|             0|                 3|        House|        3202.0|\n",
      "| 10|56.0|Female|       8054.0|       Married|                 1.0|     Bachelor's|        Unemployed|             25.61|   Rural|          3|            1.0|        8.0|       431.0|               8.0|                2|             0|                 2|        Condo|         857.0|\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|     Self-Employed| 22.59876067181393|   Urban|          3|            2.0|       17.0|       372.0|               5.0|                1|             0|                 3|        House|        2869.0|\n",
      "|  1|39.0|Female|      31678.0|      Divorced|                 3.0|       Master's|Unknown_Occupation|15.569730989408043|   Rural|          2|            1.0|       12.0|       694.0|               2.0|                2|             1|                 2|        House|        1483.0|\n",
      "|  3|21.0|  Male|     141855.0|       Married|                 2.0|     Bachelor's|Unknown_Occupation|10.938144158664583|   Rural|          1|            1.0|        0.0|       367.0|               1.0|                1|             1|                 4|    Apartment|         765.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|          Employed| 5.769783092512088|Suburban|          2|            1.0|       11.0|       398.0|               5.0|                2|             0|                 1|        Condo|         111.0|\n",
      "|  8|21.0|  Male|       1733.0|      Divorced|                 3.0|     Bachelor's|Unknown_Occupation|17.869550814826297|   Urban|          3|            1.0|       10.0|       685.0|               8.0|                2|             0|                 2|        Condo|         213.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+------------------+------------------+--------+-----------+---------------+-----------+------------+------------------+-----------------+--------------+------------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prediction results main dataframe e update koro\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Step 1: Join `old_df` with `df` (containing predictions)\n",
    "joined_df = old_df.join(df.select(\"id\", \"prediction\"), on=\"id\", how=\"left\")\n",
    "\n",
    "\n",
    "updated_df = joined_df.withColumn(\n",
    "    \"Previous Claims\",\n",
    "    when(col(\"Previous Claims\").isNull(), col(\"prediction\"))  # Use col() instead of df[\"prediction\"]\n",
    "    .otherwise(col(\"Previous Claims\"))\n",
    ").drop(\"prediction\")  # Drop extra \"prediction\" column after update\n",
    "\n",
    "# Updated dataframe dekhte chaile\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4147.showString.\n: org.apache.spark.SparkException: Multiple failures in stage materialization.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2076)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:821)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:335)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tSuppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 471.0 failed 1 times, most recent failure: Lost task 1.0 in stage 471.0 (TID 1486) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\t\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\t\tat scala.Option.foreach(Option.scala:407)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\t\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\t\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\t\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\t\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\t\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\t\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\t\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t\t... 41 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 470.0 failed 1 times, most recent failure: Lost task 6.0 in stage 470.0 (TID 1483) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mupdated_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4147.showString.\n: org.apache.spark.SparkException: Multiple failures in stage materialization.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2076)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:821)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:335)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tSuppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 471.0 failed 1 times, most recent failure: Lost task 1.0 in stage 471.0 (TID 1486) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\t\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\t\tat scala.Option.foreach(Option.scala:407)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\t\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\t\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\t\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\t\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\t\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\t\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\t\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t\t... 41 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 470.0 failed 1 times, most recent failure: Lost task 6.0 in stage 470.0 (TID 1483) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n"
     ]
    }
   ],
   "source": [
    "updated_df.filter(col(\"id\") == 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4063.showString.\n: org.apache.spark.SparkException: Multiple failures in stage materialization.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2076)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:821)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:335)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tSuppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 468.0 failed 1 times, most recent failure: Lost task 5.0 in stage 468.0 (TID 1466) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\t\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\t\tat scala.Option.foreach(Option.scala:407)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\t\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\t\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\t\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\t\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\t\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\t\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\t\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t\t... 41 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 467.0 failed 1 times, most recent failure: Lost task 6.0 in stage 467.0 (TID 1459) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [97], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mupdated_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4063.showString.\n: org.apache.spark.SparkException: Multiple failures in stage materialization.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2076)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:821)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:335)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tSuppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 468.0 failed 1 times, most recent failure: Lost task 5.0 in stage 468.0 (TID 1466) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\t\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\t\tat scala.Option.foreach(Option.scala:407)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\t\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\t\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\t\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\t\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\t\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\t\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\t\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t\t... 41 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 467.0 failed 1 times, most recent failure: Lost task 6.0 in stage 467.0 (TID 1459) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n"
     ]
    }
   ],
   "source": [
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4129.showString.\n: org.apache.spark.SparkException: Multiple failures in stage materialization.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2076)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:821)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:335)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tSuppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 465.0 failed 1 times, most recent failure: Lost task 3.0 in stage 465.0 (TID 1440) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\t\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\t\tat scala.Option.foreach(Option.scala:407)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\t\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\t\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\t\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\t\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\t\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\t\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\t\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t\t... 41 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 464.0 failed 1 times, most recent failure: Lost task 0.0 in stage 464.0 (TID 1429) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$17.hasNext(Iterator.scala:814)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVExprUtils$.extractHeader(CSVExprUtils.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVHeaderChecker.checkHeaderColumnNames(CSVHeaderChecker.scala:126)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.parseIterator(UnivocityParser.scala:446)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 48 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$17.hasNext(Iterator.scala:814)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVExprUtils$.extractHeader(CSVExprUtils.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVHeaderChecker.checkHeaderColumnNames(CSVHeaderChecker.scala:126)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.parseIterator(UnivocityParser.scala:446)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 48 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [96], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Final_df \u001b[38;5;241m=\u001b[39m updated_df\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mFinal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4129.showString.\n: org.apache.spark.SparkException: Multiple failures in stage materialization.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.multiFailuresInStageMaterializationError(QueryExecutionErrors.scala:2076)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.cleanUpAndThrowException(AdaptiveSparkPlanExec.scala:821)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:335)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tSuppressed: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 465.0 failed 1 times, most recent failure: Lost task 3.0 in stage 465.0 (TID 1440) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\t\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\t\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\t\tat scala.Option.foreach(Option.scala:407)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\t\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\t\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\t\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\t\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\t\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\t\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\t\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\t\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\t\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\t\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\t\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\t\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\tCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\t\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\t\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t\t... 41 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 464.0 failed 1 times, most recent failure: Lost task 0.0 in stage 464.0 (TID 1429) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$17.hasNext(Iterator.scala:814)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVExprUtils$.extractHeader(CSVExprUtils.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVHeaderChecker.checkHeaderColumnNames(CSVHeaderChecker.scala:126)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.parseIterator(UnivocityParser.scala:446)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 48 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$17.hasNext(Iterator.scala:814)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVExprUtils$.extractHeader(CSVExprUtils.scala:54)\r\n\tat org.apache.spark.sql.catalyst.csv.CSVHeaderChecker.checkHeaderColumnNames(CSVHeaderChecker.scala:126)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.parseIterator(UnivocityParser.scala:446)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:103)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 48 more\r\n"
     ]
    }
   ],
   "source": [
    "Final_df = updated_df.orderBy(col(\"id\"))\n",
    "Final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4111.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 460.0 failed 1 times, most recent failure: Lost task 2.0 in stage 460.0 (TID 1407) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [94], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#sorted_df.show()\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msorted_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Check the number of rows\u001b[39;00m\n\u001b[0;32m      3\u001b[0m sorted_df\u001b[38;5;241m.\u001b[39mprintSchema()   \u001b[38;5;66;03m# Check the DataFrame structure\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \n\u001b[0;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\SAIF\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4111.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 460.0 failed 1 times, most recent failure: Lost task 2.0 in stage 460.0 (TID 1407) (DESKTOP-KGDANQ7 executor driver): org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:290)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\r\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\r\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:130)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:63)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:96)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:137)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:219)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:282)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: The process cannot access the file because another process has locked a portion of the file\r\n\tat java.base/java.io.FileInputStream.readBytes(Native Method)\r\n\tat java.base/java.io.FileInputStream.read(FileInputStream.java:279)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\r\n\t... 41 more\r\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#print(sorted_df.count())  # Check the number of rows\n",
    "#sorted_df.printSchema()   # Check the DataFrame structure\n",
    "df.show(10)  # Show original DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Identify missing values\n",
    "missing_df = df2.filter(col(\"previous claims\").isNull())\n",
    "\n",
    "# Step 2: Select the same feature columns used in training (replace feature_cols with your actual list)\n",
    "feature_cols = [...]  # List of feature columns used during training\n",
    "missing_features = missing_df.select(feature_cols)\n",
    "\n",
    "# Step 3: Convert missing_features into the same format used for model training\n",
    "missing_features_transformed = feature_assembler.transform(missing_features)\n",
    "\n",
    "# Step 4: Make predictions using the trained model\n",
    "predictions = rf_model.transform(missing_features_transformed).select(\"prediction\")\n",
    "\n",
    "# Step 5: Extract predictions and replace NULL values in df2\n",
    "predicted_values = predictions.collect()  # Get the predicted values\n",
    "predicted_values = [row[\"prediction\"] for row in predicted_values]  # Convert to a list\n",
    "\n",
    "# Step 6: Add predictions back into df2\n",
    "missing_rows = missing_df.withColumn(\"previous claims\", F.lit(None))  # Preserve schema\n",
    "for i, value in enumerate(predicted_values):\n",
    "    missing_rows = missing_rows.withColumn(\"previous claims\", F.when(F.col(\"previous claims\").isNull(), value).otherwise(F.col(\"previous claims\")))\n",
    "\n",
    "# Step 7: Merge back into df2\n",
    "df2 = df2.exceptAll(missing_df).union(missing_rows)\n",
    "\n",
    "# Check if all nulls are filled\n",
    "df2.select(F.count(F.when(F.col(\"previous claims\").isNull(), 1))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|   Occupation|      Health Score|Location|  Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|   Policy Start Date|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|Gender_Index|Marital Status_Index|Education Level_Index|Occupation_Index|Location_Index|Policy Type_Index|Customer Feedback_Index|Exercise Frequency_Index|Property Type_Index|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|Self-Employed| 22.59876067181393|   Urban|      Premium|            2.0|       17.0|       372.0|               5.0|2023-12-23 15:21:...|             Poor|             0|            Weekly|        House|        2869.0|         1.0|                 2.0|                  1.0|             0.0|           2.0|              0.0|                    1.0|                     0.0|                2.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|Self-Employed|20.376093627736925|   Rural|      Premium|            0.0|        8.0|       598.0|               4.0|2021-12-01 15:21:...|             Poor|             1|            Weekly|        House|        2022.0|         0.0|                 0.0|                  1.0|             0.0|           1.0|              0.0|                    1.0|                     0.0|                2.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|     Employed| 5.769783092512088|Suburban|Comprehensive|            1.0|       11.0|       398.0|               5.0|2022-08-08 15:21:...|          Average|             0|            Rarely|        Condo|         111.0|         1.0|                 1.0|                  3.0|             1.0|           0.0|              1.0|                    0.0|                     1.0|                1.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|     Employed| 20.47371785695596|   Urban|Comprehensive|            1.0|        9.0|       635.0|               3.0|2020-08-02 15:21:...|             Poor|             0|             Daily|        Condo|          64.0|         0.0|                 2.0|                  2.0|             1.0|           2.0|              1.0|                    1.0|                     3.0|                1.0|\n",
      "| 15|18.0|  Male|       6076.0|       Married|                 2.0|    High School|     Employed| 7.442964015746718|   Urban|      Premium|            1.0|       12.0|       584.0|               5.0|2020-10-17 15:21:...|             Good|             1|           Monthly|    Apartment|         849.0|         0.0|                 2.0|                  3.0|             1.0|           2.0|              0.0|                    2.0|                     2.0|                0.0|\n",
      "| 17|34.0|Female|      45907.0|      Divorced|                 4.0|    High School|Self-Employed|  24.0596387644636|Suburban|Comprehensive|            0.0|        0.0|       694.0|               8.0|2020-12-27 15:21:...|             Poor|             1|             Daily|        Condo|         643.0|         1.0|                 1.0|                  3.0|             0.0|           0.0|              1.0|                    1.0|                     3.0|                1.0|\n",
      "| 19|40.0|Female|     123751.0|        Single|                 2.0|       Master's|Self-Employed| 24.95531647911226|Suburban|      Premium|            0.0|        8.0|       420.0|               2.0|2020-06-25 15:21:...|             Good|             1|            Rarely|        Condo|          40.0|         1.0|                 0.0|                  2.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|\n",
      "| 25|19.0|  Male|      72482.0|       Married|                 2.0|     Bachelor's|   Unemployed| 27.27941569575117|Suburban|      Premium|            0.0|       17.0|       416.0|               3.0|2021-03-10 15:21:...|             Good|             1|            Weekly|        Condo|         671.0|         0.0|                 2.0|                  1.0|             2.0|           0.0|              0.0|                    2.0|                     0.0|                1.0|\n",
      "| 27|42.0|  Male|      35123.0|       Married|                 2.0|     Bachelor's|Self-Employed| 31.39190719557818|Suburban|      Premium|            0.0|        3.0|       486.0|               4.0|2023-09-03 15:21:...|             Good|             1|            Rarely|        Condo|        2680.0|         0.0|                 2.0|                  1.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|\n",
      "| 32|58.0|  Male|       1288.0|      Divorced|                 4.0|       Master's|Self-Employed|15.171628553438794|   Urban|        Basic|            1.0|        2.0|       803.0|               2.0|2023-07-25 15:21:...|             Good|             0|            Rarely|    Apartment|        3299.0|         0.0|                 1.0|                  2.0|             0.0|           2.0|              2.0|                    2.0|                     1.0|                0.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#string indexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "to_string_indxn_columns = ['Gender', 'Marital Status', 'Education Level', 'Occupation',\n",
    "                       'Location', 'Policy Type', 'Customer Feedback', \n",
    "                       'Exercise Frequency', 'Property Type']\n",
    "\n",
    "# # index\n",
    "# indexers = [\n",
    "#     StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\").fit(df2)\n",
    "#     for col in to_string_indxn_columns\n",
    "# ]\n",
    "\n",
    "# # transform\n",
    "# df2 = indexers.transform(df2)\n",
    "\n",
    "\n",
    "# both index and transfmation together\n",
    "for col_name in to_string_indxn_columns:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_Index\" , handleInvalid=\"keep\")\n",
    "    df2 = indexer.fit(df2).transform(df2)\n",
    "\n",
    "df2.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "| id| Age|Gender|Annual Income|Marital Status|Number of Dependents|Education Level|   Occupation|      Health Score|Location|  Policy Type|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|   Policy Start Date|Customer Feedback|Smoking Status|Exercise Frequency|Property Type|Premium Amount|Gender_Index|Marital Status_Index|Education Level_Index|Occupation_Index|Location_Index|Policy Type_Index|Customer Feedback_Index|Exercise Frequency_Index|Property Type_Index|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "|  0|19.0|Female|      10049.0|       Married|                 1.0|     Bachelor's|Self-Employed| 22.59876067181393|   Urban|      Premium|            2.0|       17.0|       372.0|               5.0|2023-12-23 15:21:...|             Poor|             0|            Weekly|        House|        2869.0|         1.0|                 2.0|                  1.0|             0.0|           2.0|              0.0|                    1.0|                     0.0|                2.0|\n",
      "|  4|21.0|  Male|      39651.0|        Single|                 1.0|     Bachelor's|Self-Employed|20.376093627736925|   Rural|      Premium|            0.0|        8.0|       598.0|               4.0|2021-12-01 15:21:...|             Poor|             1|            Weekly|        House|        2022.0|         0.0|                 0.0|                  1.0|             0.0|           1.0|              0.0|                    1.0|                     0.0|                2.0|\n",
      "|  7|48.0|Female|     127237.0|      Divorced|                 2.0|    High School|     Employed| 5.769783092512088|Suburban|Comprehensive|            1.0|       11.0|       398.0|               5.0|2022-08-08 15:21:...|          Average|             0|            Rarely|        Condo|         111.0|         1.0|                 1.0|                  3.0|             1.0|           0.0|              1.0|                    0.0|                     1.0|                1.0|\n",
      "|  9|44.0|  Male|      52447.0|       Married|                 2.0|       Master's|     Employed| 20.47371785695596|   Urban|Comprehensive|            1.0|        9.0|       635.0|               3.0|2020-08-02 15:21:...|             Poor|             0|             Daily|        Condo|          64.0|         0.0|                 2.0|                  2.0|             1.0|           2.0|              1.0|                    1.0|                     3.0|                1.0|\n",
      "| 15|18.0|  Male|       6076.0|       Married|                 2.0|    High School|     Employed| 7.442964015746718|   Urban|      Premium|            1.0|       12.0|       584.0|               5.0|2020-10-17 15:21:...|             Good|             1|           Monthly|    Apartment|         849.0|         0.0|                 2.0|                  3.0|             1.0|           2.0|              0.0|                    2.0|                     2.0|                0.0|\n",
      "| 17|34.0|Female|      45907.0|      Divorced|                 4.0|    High School|Self-Employed|  24.0596387644636|Suburban|Comprehensive|            0.0|        0.0|       694.0|               8.0|2020-12-27 15:21:...|             Poor|             1|             Daily|        Condo|         643.0|         1.0|                 1.0|                  3.0|             0.0|           0.0|              1.0|                    1.0|                     3.0|                1.0|\n",
      "| 19|40.0|Female|     123751.0|        Single|                 2.0|       Master's|Self-Employed| 24.95531647911226|Suburban|      Premium|            0.0|        8.0|       420.0|               2.0|2020-06-25 15:21:...|             Good|             1|            Rarely|        Condo|          40.0|         1.0|                 0.0|                  2.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|\n",
      "| 25|19.0|  Male|      72482.0|       Married|                 2.0|     Bachelor's|   Unemployed| 27.27941569575117|Suburban|      Premium|            0.0|       17.0|       416.0|               3.0|2021-03-10 15:21:...|             Good|             1|            Weekly|        Condo|         671.0|         0.0|                 2.0|                  1.0|             2.0|           0.0|              0.0|                    2.0|                     0.0|                1.0|\n",
      "| 27|42.0|  Male|      35123.0|       Married|                 2.0|     Bachelor's|Self-Employed| 31.39190719557818|Suburban|      Premium|            0.0|        3.0|       486.0|               4.0|2023-09-03 15:21:...|             Good|             1|            Rarely|        Condo|        2680.0|         0.0|                 2.0|                  1.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|\n",
      "| 32|58.0|  Male|       1288.0|      Divorced|                 4.0|       Master's|Self-Employed|15.171628553438794|   Urban|        Basic|            1.0|        2.0|       803.0|               2.0|2023-07-25 15:21:...|             Good|             0|            Rarely|    Apartment|        3299.0|         0.0|                 1.0|                  2.0|             0.0|           2.0|              2.0|                    2.0|                     1.0|                0.0|\n",
      "+---+----+------+-------------+--------------+--------------------+---------------+-------------+------------------+--------+-------------+---------------+-----------+------------+------------------+--------------------+-----------------+--------------+------------------+-------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed = df2.drop(*to_string_indxn_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|   Policy Start Date|Smoking Status|Premium Amount|Gender_Index|Marital Status_Index|Education Level_Index|Occupation_Index|Location_Index|Policy Type_Index|Customer Feedback_Index|Exercise Frequency_Index|Property Type_Index|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "|  0|19.0|      10049.0|                 1.0| 22.59876067181393|            2.0|       17.0|       372.0|               5.0|2023-12-23 15:21:...|             0|        2869.0|         1.0|                 2.0|                  1.0|             0.0|           2.0|              0.0|                    1.0|                     0.0|                2.0|\n",
      "|  4|21.0|      39651.0|                 1.0|20.376093627736925|            0.0|        8.0|       598.0|               4.0|2021-12-01 15:21:...|             1|        2022.0|         0.0|                 0.0|                  1.0|             0.0|           1.0|              0.0|                    1.0|                     0.0|                2.0|\n",
      "|  7|48.0|     127237.0|                 2.0| 5.769783092512088|            1.0|       11.0|       398.0|               5.0|2022-08-08 15:21:...|             0|         111.0|         1.0|                 1.0|                  3.0|             1.0|           0.0|              1.0|                    0.0|                     1.0|                1.0|\n",
      "|  9|44.0|      52447.0|                 2.0| 20.47371785695596|            1.0|        9.0|       635.0|               3.0|2020-08-02 15:21:...|             0|          64.0|         0.0|                 2.0|                  2.0|             1.0|           2.0|              1.0|                    1.0|                     3.0|                1.0|\n",
      "| 15|18.0|       6076.0|                 2.0| 7.442964015746718|            1.0|       12.0|       584.0|               5.0|2020-10-17 15:21:...|             1|         849.0|         0.0|                 2.0|                  3.0|             1.0|           2.0|              0.0|                    2.0|                     2.0|                0.0|\n",
      "| 17|34.0|      45907.0|                 4.0|  24.0596387644636|            0.0|        0.0|       694.0|               8.0|2020-12-27 15:21:...|             1|         643.0|         1.0|                 1.0|                  3.0|             0.0|           0.0|              1.0|                    1.0|                     3.0|                1.0|\n",
      "| 19|40.0|     123751.0|                 2.0| 24.95531647911226|            0.0|        8.0|       420.0|               2.0|2020-06-25 15:21:...|             1|          40.0|         1.0|                 0.0|                  2.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|\n",
      "| 25|19.0|      72482.0|                 2.0| 27.27941569575117|            0.0|       17.0|       416.0|               3.0|2021-03-10 15:21:...|             1|         671.0|         0.0|                 2.0|                  1.0|             2.0|           0.0|              0.0|                    2.0|                     0.0|                1.0|\n",
      "| 27|42.0|      35123.0|                 2.0| 31.39190719557818|            0.0|        3.0|       486.0|               4.0|2023-09-03 15:21:...|             1|        2680.0|         0.0|                 2.0|                  1.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|\n",
      "| 32|58.0|       1288.0|                 4.0|15.171628553438794|            1.0|        2.0|       803.0|               2.0|2023-07-25 15:21:...|             0|        3299.0|         0.0|                 1.0|                  2.0|             0.0|           2.0|              2.0|                    2.0|                     1.0|                0.0|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model training for occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting df2 into 80% training and 20% testing\n",
    "train_df, test_df = df_indexed.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 181:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Smoking Status|Premium Amount|Gender_Index|Marital Status_Index|Education Level_Index|Occupation_Index|Location_Index|Policy Type_Index|Customer Feedback_Index|Exercise Frequency_Index|Property Type_Index|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "|  0|19.0|      10049.0|                 1.0| 22.59876067181393|            2.0|       17.0|       372.0|               5.0|             0|        2869.0|         1.0|                 2.0|                  1.0|             0.0|           2.0|              0.0|                    1.0|                     0.0|                2.0|\n",
      "|  4|21.0|      39651.0|                 1.0|20.376093627736925|            0.0|        8.0|       598.0|               4.0|             1|        2022.0|         0.0|                 0.0|                  1.0|             0.0|           1.0|              0.0|                    1.0|                     0.0|                2.0|\n",
      "|  9|44.0|      52447.0|                 2.0| 20.47371785695596|            1.0|        9.0|       635.0|               3.0|             0|          64.0|         0.0|                 2.0|                  2.0|             1.0|           2.0|              1.0|                    1.0|                     3.0|                1.0|\n",
      "| 15|18.0|       6076.0|                 2.0| 7.442964015746718|            1.0|       12.0|       584.0|               5.0|             1|         849.0|         0.0|                 2.0|                  3.0|             1.0|           2.0|              0.0|                    2.0|                     2.0|                0.0|\n",
      "| 17|34.0|      45907.0|                 4.0|  24.0596387644636|            0.0|        0.0|       694.0|               8.0|             1|         643.0|         1.0|                 1.0|                  3.0|             0.0|           0.0|              1.0|                    1.0|                     3.0|                1.0|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'Age',\n",
       " 'Annual Income',\n",
       " 'Number of Dependents',\n",
       " 'Health Score',\n",
       " 'Previous Claims',\n",
       " 'Vehicle Age',\n",
       " 'Credit Score',\n",
       " 'Insurance Duration',\n",
       " 'Smoking Status',\n",
       " 'Premium Amount',\n",
       " 'Gender_Index',\n",
       " 'Marital Status_Index',\n",
       " 'Education Level_Index',\n",
       " 'Occupation_Index',\n",
       " 'Location_Index',\n",
       " 'Policy Type_Index',\n",
       " 'Customer Feedback_Index',\n",
       " 'Exercise Frequency_Index',\n",
       " 'Property Type_Index']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = ['id',\n",
    " 'Age',\n",
    " 'Annual Income',\n",
    " 'Number of Dependents',\n",
    " 'Health Score',\n",
    " 'Previous Claims',\n",
    " 'Vehicle Age',\n",
    " 'Credit Score',\n",
    " 'Insurance Duration',\n",
    " 'Smoking Status',\n",
    " 'Premium Amount',\n",
    " 'Gender_Index',\n",
    " 'Marital Status_Index',\n",
    " 'Education Level_Index',\n",
    " 'Location_Index',\n",
    " 'Policy Type_Index',\n",
    " 'Customer Feedback_Index',\n",
    " 'Exercise Frequency_Index',\n",
    " 'Property Type_Index']\n",
    "\n",
    "label_col = 'Occupation_Index'\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorAssembler_e7308c298679\n"
     ]
    }
   ],
   "source": [
    "print(assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = assembler.transform(train_df)\n",
    "test_data = assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 182:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Smoking Status|Premium Amount|Gender_Index|Marital Status_Index|Education Level_Index|Occupation_Index|Location_Index|Policy Type_Index|Customer Feedback_Index|Exercise Frequency_Index|Property Type_Index|            features|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+\n",
      "|  0|19.0|      10049.0|                 1.0| 22.59876067181393|            2.0|       17.0|       372.0|               5.0|             0|        2869.0|         1.0|                 2.0|                  1.0|             0.0|           2.0|              0.0|                    1.0|                     0.0|                2.0|[0.0,19.0,10049.0...|\n",
      "|  4|21.0|      39651.0|                 1.0|20.376093627736925|            0.0|        8.0|       598.0|               4.0|             1|        2022.0|         0.0|                 0.0|                  1.0|             0.0|           1.0|              0.0|                    1.0|                     0.0|                2.0|[4.0,21.0,39651.0...|\n",
      "|  9|44.0|      52447.0|                 2.0| 20.47371785695596|            1.0|        9.0|       635.0|               3.0|             0|          64.0|         0.0|                 2.0|                  2.0|             1.0|           2.0|              1.0|                    1.0|                     3.0|                1.0|[9.0,44.0,52447.0...|\n",
      "| 15|18.0|       6076.0|                 2.0| 7.442964015746718|            1.0|       12.0|       584.0|               5.0|             1|         849.0|         0.0|                 2.0|                  3.0|             1.0|           2.0|              0.0|                    2.0|                     2.0|                0.0|[15.0,18.0,6076.0...|\n",
      "| 17|34.0|      45907.0|                 4.0|  24.0596387644636|            0.0|        0.0|       694.0|               8.0|             1|         643.0|         1.0|                 1.0|                  3.0|             0.0|           0.0|              1.0|                    1.0|                     3.0|                1.0|[17.0,34.0,45907....|\n",
      "| 25|19.0|      72482.0|                 2.0| 27.27941569575117|            0.0|       17.0|       416.0|               3.0|             1|         671.0|         0.0|                 2.0|                  1.0|             2.0|           0.0|              0.0|                    2.0|                     0.0|                1.0|[25.0,19.0,72482....|\n",
      "| 32|58.0|       1288.0|                 4.0|15.171628553438794|            1.0|        2.0|       803.0|               2.0|             0|        3299.0|         0.0|                 1.0|                  2.0|             0.0|           2.0|              2.0|                    2.0|                     1.0|                0.0|[32.0,58.0,1288.0...|\n",
      "| 40|59.0|       1717.0|                 1.0|23.900048744874688|            0.0|        9.0|       382.0|               6.0|             0|        1813.0|         0.0|                 1.0|                  1.0|             2.0|           2.0|              2.0|                    2.0|                     0.0|                2.0|[40.0,59.0,1717.0...|\n",
      "| 41|46.0|      14053.0|                 0.0| 18.56935672012438|            3.0|       14.0|       469.0|               5.0|             0|        1211.0|         0.0|                 2.0|                  3.0|             2.0|           0.0|              1.0|                    2.0|                     3.0|                1.0|[41.0,46.0,14053....|\n",
      "| 47|54.0|      29076.0|                 1.0| 36.83989236833922|            0.0|        2.0|       685.0|               7.0|             1|        1645.0|         1.0|                 2.0|                  3.0|             1.0|           1.0|              1.0|                    0.0|                     3.0|                1.0|[47.0,54.0,29076....|\n",
      "| 49|45.0|      11584.0|                 4.0| 29.18189999982652|            1.0|        3.0|       836.0|               4.0|             0|        1309.0|         0.0|                 2.0|                  1.0|             0.0|           2.0|              2.0|                    2.0|                     3.0|                0.0|[49.0,45.0,11584....|\n",
      "| 64|48.0|      94904.0|                 1.0|  45.9792179435217|            0.0|        3.0|       426.0|               1.0|             0|         861.0|         1.0|                 1.0|                  1.0|             0.0|           2.0|              1.0|                    0.0|                     2.0|                0.0|[64.0,48.0,94904....|\n",
      "| 65|54.0|       6841.0|                 1.0|14.893776085341344|            0.0|       15.0|       331.0|               2.0|             0|        2160.0|         0.0|                 1.0|                  0.0|             2.0|           2.0|              0.0|                    1.0|                     3.0|                1.0|[65.0,54.0,6841.0...|\n",
      "| 68|53.0|      21313.0|                 1.0|21.161960698769622|            2.0|        7.0|       321.0|               1.0|             0|        2453.0|         0.0|                 0.0|                  3.0|             0.0|           1.0|              0.0|                    2.0|                     3.0|                0.0|[68.0,53.0,21313....|\n",
      "| 74|23.0|       7515.0|                 0.0| 22.52720580349861|            2.0|        4.0|       418.0|               9.0|             0|        3564.0|         0.0|                 0.0|                  2.0|             0.0|           0.0|              1.0|                    0.0|                     3.0|                1.0|[74.0,23.0,7515.0...|\n",
      "| 91|63.0|        915.0|                 1.0| 22.96414544532541|            1.0|       17.0|       584.0|               7.0|             0|        3664.0|         1.0|                 0.0|                  0.0|             1.0|           1.0|              0.0|                    1.0|                     2.0|                2.0|[91.0,63.0,915.0,...|\n",
      "| 92|53.0|       1170.0|                 0.0| 38.59663905255338|            0.0|       13.0|       791.0|               6.0|             0|        2096.0|         1.0|                 0.0|                  0.0|             1.0|           1.0|              0.0|                    0.0|                     3.0|                1.0|[92.0,53.0,1170.0...|\n",
      "| 95|32.0|      27962.0|                 2.0| 45.39134918171405|            0.0|        8.0|       438.0|               2.0|             1|        2344.0|         1.0|                 0.0|                  2.0|             0.0|           2.0|              2.0|                    0.0|                     1.0|                1.0|[95.0,32.0,27962....|\n",
      "| 98|43.0|       5813.0|                 2.0|29.441474404927497|            1.0|       13.0|       748.0|               5.0|             1|         837.0|         1.0|                 1.0|                  1.0|             2.0|           2.0|              1.0|                    1.0|                     0.0|                0.0|[98.0,43.0,5813.0...|\n",
      "|102|63.0|      31400.0|                 4.0|15.936011527150502|            0.0|        0.0|       593.0|               2.0|             0|         657.0|         1.0|                 2.0|                  1.0|             1.0|           0.0|              2.0|                    0.0|                     2.0|                0.0|[102.0,63.0,31400...|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=label_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/03 01:56:25 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 271:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Smoking Status|Premium Amount|Gender_Index|Marital Status_Index|Education Level_Index|Occupation_Index|Location_Index|Policy Type_Index|Customer Feedback_Index|Exercise Frequency_Index|Property Type_Index|            features|       rawPrediction|         probability|prediction|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  7|48.0|     127237.0|                 2.0| 5.769783092512088|            1.0|       11.0|       398.0|               5.0|             0|         111.0|         1.0|                 1.0|                  3.0|             1.0|           0.0|              1.0|                    0.0|                     1.0|                1.0|[7.0,48.0,127237....|[5.48334314597443...|[0.33822258703501...|       0.0|\n",
      "| 19|40.0|     123751.0|                 2.0| 24.95531647911226|            0.0|        8.0|       420.0|               2.0|             1|          40.0|         1.0|                 0.0|                  2.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|[19.0,40.0,123751...|[5.46540164304188...|[0.33209546672309...|       1.0|\n",
      "| 27|42.0|      35123.0|                 2.0| 31.39190719557818|            0.0|        3.0|       486.0|               4.0|             1|        2680.0|         0.0|                 2.0|                  1.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|[27.0,42.0,35123....|[5.44457254755097...|[0.32524517173051...|       1.0|\n",
      "| 48|34.0|      12221.0|                 0.0| 37.82939868635599|            1.0|        1.0|       794.0|               1.0|             1|        1486.0|         1.0|                 1.0|                  3.0|             1.0|           1.0|              1.0|                    0.0|                     0.0|                2.0|[48.0,34.0,12221....|[5.46607127352648...|[0.33212847380352...|       1.0|\n",
      "| 85|61.0|      19542.0|                 1.0| 20.78171123924755|            1.0|       17.0|       371.0|               5.0|             0|         903.0|         1.0|                 2.0|                  1.0|             1.0|           2.0|              1.0|                    1.0|                     2.0|                0.0|[85.0,61.0,19542....|[5.47463257895794...|[0.33531706522856...|       0.0|\n",
      "| 97|36.0|       2693.0|                 3.0|14.011567635774558|            1.0|       18.0|       310.0|               4.0|             0|        1078.0|         1.0|                 2.0|                  2.0|             1.0|           1.0|              0.0|                    1.0|                     2.0|                1.0|[97.0,36.0,2693.0...|[5.47636846081441...|[0.33558742988844...|       1.0|\n",
      "|111|40.0|      14079.0|                 2.0| 36.56738626656398|            0.0|       17.0|       602.0|               3.0|             0|        1461.0|         0.0|                 0.0|                  0.0|             2.0|           0.0|              0.0|                    0.0|                     2.0|                0.0|(19,[0,1,2,3,4,6,...|[5.48357655790795...|[0.33815314152319...|       0.0|\n",
      "|127|20.0|      54532.0|                 4.0| 50.09765165719381|            1.0|       16.0|       638.0|               4.0|             1|        2034.0|         0.0|                 1.0|                  3.0|             1.0|           1.0|              0.0|                    2.0|                     1.0|                2.0|[127.0,20.0,54532...|[5.45714520574069...|[0.32930842110654...|       1.0|\n",
      "|157|33.0|       3098.0|                 0.0|12.296741379273572|            3.0|       14.0|       592.0|               7.0|             1|        1033.0|         1.0|                 0.0|                  2.0|             1.0|           1.0|              2.0|                    2.0|                     0.0|                2.0|[157.0,33.0,3098....|[5.48333102190828...|[0.33771967030093...|       1.0|\n",
      "|160|58.0|       2592.0|                 3.0| 30.48454324490176|            3.0|        6.0|       392.0|               4.0|             1|        3040.0|         1.0|                 1.0|                  1.0|             1.0|           1.0|              1.0|                    2.0|                     2.0|                1.0|[160.0,58.0,2592....|[5.45026464346086...|[0.32700415404963...|       1.0|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 272:============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize the evaluator\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Occupation_Index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: int, Age: double, Annual Income: double, Number of Dependents: double, Health Score: double, Previous Claims: double, Vehicle Age: double, Credit Score: double, Insurance Duration: double, Smoking Status: int, Premium Amount: double, Gender_Index: double, Marital Status_Index: double, Education Level_Index: double, Occupation_Index: double, Location_Index: double, Policy Type_Index: double, Customer Feedback_Index: double, Exercise Frequency_Index: double, Property Type_Index: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]\n"
     ]
    }
   ],
   "source": [
    "print(lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\",labelCol=\"Occupation_Index\")\n",
    "rf_model = rf.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 291:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Smoking Status|Premium Amount|Gender_Index|Marital Status_Index|Education Level_Index|Occupation_Index|Location_Index|Policy Type_Index|Customer Feedback_Index|Exercise Frequency_Index|Property Type_Index|            features|       rawPrediction|         probability|prediction|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  7|48.0|     127237.0|                 2.0| 5.769783092512088|            1.0|       11.0|       398.0|               5.0|             0|         111.0|         1.0|                 1.0|                  3.0|             1.0|           0.0|              1.0|                    0.0|                     1.0|                1.0|[7.0,48.0,127237....|[6.65559307505903...|[0.33277965375295...|       2.0|\n",
      "| 19|40.0|     123751.0|                 2.0| 24.95531647911226|            0.0|        8.0|       420.0|               2.0|             1|          40.0|         1.0|                 0.0|                  2.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|[19.0,40.0,123751...|[6.68752295982825...|[0.33437614799141...|       0.0|\n",
      "| 27|42.0|      35123.0|                 2.0| 31.39190719557818|            0.0|        3.0|       486.0|               4.0|             1|        2680.0|         0.0|                 2.0|                  1.0|             0.0|           0.0|              0.0|                    2.0|                     1.0|                1.0|[27.0,42.0,35123....|[6.80043218955721...|[0.34002160947786...|       0.0|\n",
      "| 48|34.0|      12221.0|                 0.0| 37.82939868635599|            1.0|        1.0|       794.0|               1.0|             1|        1486.0|         1.0|                 1.0|                  3.0|             1.0|           1.0|              1.0|                    0.0|                     0.0|                2.0|[48.0,34.0,12221....|[6.64848089603909...|[0.33242404480195...|       1.0|\n",
      "| 85|61.0|      19542.0|                 1.0| 20.78171123924755|            1.0|       17.0|       371.0|               5.0|             0|         903.0|         1.0|                 2.0|                  1.0|             1.0|           2.0|              1.0|                    1.0|                     2.0|                0.0|[85.0,61.0,19542....|[6.69798859088262...|[0.33489942954413...|       0.0|\n",
      "| 97|36.0|       2693.0|                 3.0|14.011567635774558|            1.0|       18.0|       310.0|               4.0|             0|        1078.0|         1.0|                 2.0|                  2.0|             1.0|           1.0|              0.0|                    1.0|                     2.0|                1.0|[97.0,36.0,2693.0...|[6.79418610727414...|[0.33970930536370...|       0.0|\n",
      "|111|40.0|      14079.0|                 2.0| 36.56738626656398|            0.0|       17.0|       602.0|               3.0|             0|        1461.0|         0.0|                 0.0|                  0.0|             2.0|           0.0|              0.0|                    0.0|                     2.0|                0.0|(19,[0,1,2,3,4,6,...|[6.67841678270570...|[0.33392083913528...|       1.0|\n",
      "|127|20.0|      54532.0|                 4.0| 50.09765165719381|            1.0|       16.0|       638.0|               4.0|             1|        2034.0|         0.0|                 1.0|                  3.0|             1.0|           1.0|              0.0|                    2.0|                     1.0|                2.0|[127.0,20.0,54532...|[6.72933748559616...|[0.33646687427980...|       1.0|\n",
      "|157|33.0|       3098.0|                 0.0|12.296741379273572|            3.0|       14.0|       592.0|               7.0|             1|        1033.0|         1.0|                 0.0|                  2.0|             1.0|           1.0|              2.0|                    2.0|                     0.0|                2.0|[157.0,33.0,3098....|[6.74598424077689...|[0.33729921203884...|       1.0|\n",
      "|160|58.0|       2592.0|                 3.0| 30.48454324490176|            3.0|        6.0|       392.0|               4.0|             1|        3040.0|         1.0|                 1.0|                  1.0|             1.0|           1.0|              1.0|                    2.0|                     2.0|                1.0|[160.0,58.0,2592....|[6.71693112184410...|[0.33584655609220...|       0.0|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 288:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 33.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Occupation_Index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Random Forest Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 290:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+\n",
      "| id| Age|Annual Income|Number of Dependents|      Health Score|Previous Claims|Vehicle Age|Credit Score|Insurance Duration|Smoking Status|Premium Amount|Gender_Index|Marital Status_Index|Education Level_Index|Occupation_Index|Location_Index|Policy Type_Index|Customer Feedback_Index|Exercise Frequency_Index|Property Type_Index|            features|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+\n",
      "|  0|19.0|      10049.0|                 1.0| 22.59876067181393|            2.0|       17.0|       372.0|               5.0|             0|        2869.0|         1.0|                 2.0|                  1.0|             0.0|           2.0|              0.0|                    1.0|                     0.0|                2.0|[0.0,19.0,10049.0...|\n",
      "|  4|21.0|      39651.0|                 1.0|20.376093627736925|            0.0|        8.0|       598.0|               4.0|             1|        2022.0|         0.0|                 0.0|                  1.0|             0.0|           1.0|              0.0|                    1.0|                     0.0|                2.0|[4.0,21.0,39651.0...|\n",
      "|  9|44.0|      52447.0|                 2.0| 20.47371785695596|            1.0|        9.0|       635.0|               3.0|             0|          64.0|         0.0|                 2.0|                  2.0|             1.0|           2.0|              1.0|                    1.0|                     3.0|                1.0|[9.0,44.0,52447.0...|\n",
      "| 15|18.0|       6076.0|                 2.0| 7.442964015746718|            1.0|       12.0|       584.0|               5.0|             1|         849.0|         0.0|                 2.0|                  3.0|             1.0|           2.0|              0.0|                    2.0|                     2.0|                0.0|[15.0,18.0,6076.0...|\n",
      "| 17|34.0|      45907.0|                 4.0|  24.0596387644636|            0.0|        0.0|       694.0|               8.0|             1|         643.0|         1.0|                 1.0|                  3.0|             0.0|           0.0|              1.0|                    1.0|                     3.0|                1.0|[17.0,34.0,45907....|\n",
      "| 25|19.0|      72482.0|                 2.0| 27.27941569575117|            0.0|       17.0|       416.0|               3.0|             1|         671.0|         0.0|                 2.0|                  1.0|             2.0|           0.0|              0.0|                    2.0|                     0.0|                1.0|[25.0,19.0,72482....|\n",
      "| 32|58.0|       1288.0|                 4.0|15.171628553438794|            1.0|        2.0|       803.0|               2.0|             0|        3299.0|         0.0|                 1.0|                  2.0|             0.0|           2.0|              2.0|                    2.0|                     1.0|                0.0|[32.0,58.0,1288.0...|\n",
      "| 40|59.0|       1717.0|                 1.0|23.900048744874688|            0.0|        9.0|       382.0|               6.0|             0|        1813.0|         0.0|                 1.0|                  1.0|             2.0|           2.0|              2.0|                    2.0|                     0.0|                2.0|[40.0,59.0,1717.0...|\n",
      "| 41|46.0|      14053.0|                 0.0| 18.56935672012438|            3.0|       14.0|       469.0|               5.0|             0|        1211.0|         0.0|                 2.0|                  3.0|             2.0|           0.0|              1.0|                    2.0|                     3.0|                1.0|[41.0,46.0,14053....|\n",
      "| 47|54.0|      29076.0|                 1.0| 36.83989236833922|            0.0|        2.0|       685.0|               7.0|             1|        1645.0|         1.0|                 2.0|                  3.0|             1.0|           1.0|              1.0|                    0.0|                     3.0|                1.0|[47.0,54.0,29076....|\n",
      "| 49|45.0|      11584.0|                 4.0| 29.18189999982652|            1.0|        3.0|       836.0|               4.0|             0|        1309.0|         0.0|                 2.0|                  1.0|             0.0|           2.0|              2.0|                    2.0|                     3.0|                0.0|[49.0,45.0,11584....|\n",
      "| 64|48.0|      94904.0|                 1.0|  45.9792179435217|            0.0|        3.0|       426.0|               1.0|             0|         861.0|         1.0|                 1.0|                  1.0|             0.0|           2.0|              1.0|                    0.0|                     2.0|                0.0|[64.0,48.0,94904....|\n",
      "| 65|54.0|       6841.0|                 1.0|14.893776085341344|            0.0|       15.0|       331.0|               2.0|             0|        2160.0|         0.0|                 1.0|                  0.0|             2.0|           2.0|              0.0|                    1.0|                     3.0|                1.0|[65.0,54.0,6841.0...|\n",
      "| 68|53.0|      21313.0|                 1.0|21.161960698769622|            2.0|        7.0|       321.0|               1.0|             0|        2453.0|         0.0|                 0.0|                  3.0|             0.0|           1.0|              0.0|                    2.0|                     3.0|                0.0|[68.0,53.0,21313....|\n",
      "| 74|23.0|       7515.0|                 0.0| 22.52720580349861|            2.0|        4.0|       418.0|               9.0|             0|        3564.0|         0.0|                 0.0|                  2.0|             0.0|           0.0|              1.0|                    0.0|                     3.0|                1.0|[74.0,23.0,7515.0...|\n",
      "| 91|63.0|        915.0|                 1.0| 22.96414544532541|            1.0|       17.0|       584.0|               7.0|             0|        3664.0|         1.0|                 0.0|                  0.0|             1.0|           1.0|              0.0|                    1.0|                     2.0|                2.0|[91.0,63.0,915.0,...|\n",
      "| 92|53.0|       1170.0|                 0.0| 38.59663905255338|            0.0|       13.0|       791.0|               6.0|             0|        2096.0|         1.0|                 0.0|                  0.0|             1.0|           1.0|              0.0|                    0.0|                     3.0|                1.0|[92.0,53.0,1170.0...|\n",
      "| 95|32.0|      27962.0|                 2.0| 45.39134918171405|            0.0|        8.0|       438.0|               2.0|             1|        2344.0|         1.0|                 0.0|                  2.0|             0.0|           2.0|              2.0|                    0.0|                     1.0|                1.0|[95.0,32.0,27962....|\n",
      "| 98|43.0|       5813.0|                 2.0|29.441474404927497|            1.0|       13.0|       748.0|               5.0|             1|         837.0|         1.0|                 1.0|                  1.0|             2.0|           2.0|              1.0|                    1.0|                     0.0|                0.0|[98.0,43.0,5813.0...|\n",
      "|102|63.0|      31400.0|                 4.0|15.936011527150502|            0.0|        0.0|       593.0|               2.0|             0|         657.0|         1.0|                 2.0|                  1.0|             1.0|           0.0|              2.0|                    0.0|                     2.0|                0.0|[102.0,63.0,31400...|\n",
      "+---+----+-------------+--------------------+------------------+---------------+-----------+------------+------------------+--------------+--------------+------------+--------------------+---------------------+----------------+--------------+-----------------+-----------------------+------------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start filling the Null using the (rf_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occupation > string indexer but\n",
    "\n",
    "we have to fill the Null with (unknown) \n",
    "\n",
    "then we'll do it but we need the same indexer values as before(during the time of training)\n",
    "\n",
    "> here comes the problem which is (string indexer) miss match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.printSchema() #working \n",
    "# test_df.show(5) #not working\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Age', 'Gender', 'Annual Income', 'Marital Status', 'Number of Dependents', 'Education Level', 'Occupation', 'Health Score', 'Location', 'Policy Type', 'Previous Claims', 'Vehicle Age', 'Credit Score', 'Insurance Duration', 'Policy Start Date', 'Customer Feedback', 'Smoking Status', 'Exercise Frequency', 'Property Type', 'Premium Amount']\n"
     ]
    }
   ],
   "source": [
    "# column_names = df_train.columns\n",
    "# print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   # Step 2: Define Schema (based on the data sample)\n",
    "#     schema = StructType([\n",
    "#         StructField(\"id\", IntegerType(), True),\n",
    "#         StructField(\"Age\", DoubleType(), True),\n",
    "#         StructField(\"Gender\", StringType(), True),\n",
    "#         StructField(\"Annual_Income\", DoubleType(), True),\n",
    "#         StructField(\"Marital_Status\", StringType(), True),\n",
    "#         StructField(\"Number_of_Dependents\", DoubleType(), True),\n",
    "#         StructField(\"Education_Level\", StringType(), True),\n",
    "#         StructField(\"Occupation\", StringType(), True),\n",
    "#         StructField(\"Health_Score\", DoubleType(), True),\n",
    "#         StructField(\"Location\", StringType(), True),\n",
    "#         StructField(\"Policy_Type\", StringType(), True),\n",
    "#         StructField(\"Previous_Claims\", DoubleType(), True),\n",
    "#         StructField(\"Vehicle_Age\", DoubleType(), True),\n",
    "#         StructField(\"Credit_Score\", DoubleType(), True),\n",
    "#         StructField(\"Insurance_Duration\", DoubleType(), True),\n",
    "#         StructField(\"Policy_Start_Date\", StringType(), True),\n",
    "#         StructField(\"Customer_Feedback\", StringType(), True),\n",
    "#         StructField(\"Smoking_Status\", StringType(), True),\n",
    "#         StructField(\"Exercise_Frequency\", StringType(), True),\n",
    "#         StructField(\"Property_Type\", StringType(), True),\n",
    "#         StructField(\"Premium_Amount\", DoubleType(), True)\n",
    "#     ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
